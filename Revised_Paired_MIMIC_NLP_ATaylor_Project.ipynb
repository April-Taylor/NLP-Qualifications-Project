{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/April-Taylor/NLP-Qualifications-Project/blob/main/Revised_Paired_MIMIC_NLP_ATaylor_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["# üîß Initial Project Setup (Run Once per Project)"],"metadata":{"id":"z-2Oq724hBu_"}},{"cell_type":"markdown","source":["This notebook sets up GitHub integration for a Google Colab project.\n","\n","It connects Colab to Google Drive and a GitHub repository, allowing work done\n","in Colab to be saved locally in Drive and versioned via GitHub. It also securely\n","loads any API keys or tokens needed (e.g., GitHub, WandB, UMLS, HuggingFace) using Colab's Secrets tool.\n","\n","\n","\n","**üìÅ Important:**\n","\n","\n","You must:\n","- Have a GitHub account and pre-created repository\n","- Use the same GitHub repo name in the USER INPUT section\n","- Place your working notebooks inside /notebooks subfolder of the Colab Google Drive. This ensures your work is versioned by Git and automatically pushed to GitHub at the end of each session.\n","- Create API tokens\n","\n","‚ö†Ô∏è To ensure autosave works correctly, always open this notebook directly from Google Drive ‚Äî not from ‚ÄúRecent,‚Äù ‚ÄúGitHub,‚Äù or an uploaded file.\n","\n","üîê Required Secrets:\n","- GitHubToken\n","- wandb (optional)\n","- UMLS (optional)\n","- HF_TOKEN (optional)\n","\n","‚ö†Ô∏è Make sure you have created these API tokens on their respective platforms and added them via the Colab Secrets UI (üîë icon on the left sidebar ‚Üí Add new secret). The GitHub Token is required for proper setup. The other secrets are optional depending on the project needs.\n","\n","\n","\n","üöÄ **To initiate a new project**\n","1.   Update the user inputs\n","2.   Run this notebook through RUN SETUP\n","\n","\n","üìå Note: This entire section should be run only once when starting a new project. It initializes the GitHub connection, sets up the folder structure, and loads your Colab secrets.\n","\n","For regular use:\n","\n","*   At the start of each new Colab session, run the \"Session Start: Sync from GitHub\" cell to mount Google Drive, load secrets, and pull any updates.\n","\n","*   At the end of your session, run the \"Push Changes to GitHub\" cell to save your work back to the repository."],"metadata":{"id":"F4ksWmzsgnVo"}},{"cell_type":"markdown","source":["## USER INPUT\n","\n"],"metadata":{"id":"nFt3BzLgg5Ji"}},{"cell_type":"code","source":["# Only update this section to reuse across projects\n","GITHUB_USER = \"April-Taylor\"\n","REPO_NAME = \"NLP-Qualifications-Project\"\n","USER_EMAIL = \"Taylor.april.dawn@gmail.com\"\n","USER_FULLNAME = \"April Taylor\"\n","REPO_DESCRIPTION = \"\"\"\n","This repository contains code, data, and deliverables related to NLP-based adverse event detection integrating structured glucose-insulin time series and unstructured clinical text. Designed for modular reuse across projects.\n","\"\"\"\n","DRIVE_BASE = \"MyDrive/ColabRepos\"  # ‚úÖ Adjust this if your folder structure is different\n","\n","from google.colab import drive, userdata\n","import os\n","\n"],"metadata":{"id":"sCQ73vsag2HB","executionInfo":{"status":"ok","timestamp":1753823321481,"user_tz":240,"elapsed":2,"user":{"displayName":"April Taylor","userId":"05794515634141358651"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## FUNCTIONALIZED SETUP\n","Project setup function - run only once per project"],"metadata":{"id":"ySAnDyomh73V"}},{"cell_type":"code","source":["def setup_colab_project(github_user, repo_name, user_email, user_fullname, repo_description, drive_base):\n","    project_path = f\"/content/drive/{drive_base}/{repo_name}\"\n","    repo_url = f\"https://github.com/{github_user}/{repo_name}.git\"\n","\n","    # Mount Google Drive\n","    if not os.path.ismount(\"/content/drive\"):\n","        drive.mount('/content/drive')\n","    os.makedirs(f\"/content/drive/{drive_base}\", exist_ok=True)\n","\n","    # Git identity setup\n","    !git config --global user.email \"{user_email}\"\n","    !git config --global user.name \"{user_fullname}\"\n","\n","    # Clone repo if not already in Drive\n","    if not os.path.exists(project_path):\n","        !git clone {repo_url} \"{project_path}\"\n","\n","    %cd \"{project_path}\"\n","\n","    # Load secrets securely\n","    os.environ['GITHUB_TOKEN'] = userdata.get('GitHubToken')\n","    os.environ['WANDB_API_KEY'] = userdata.get('wandb')\n","    os.environ['UMLS_API_KEY'] = userdata.get('UMLS')\n","    os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n","\n","    # Create folder structure\n","    for folder in [\"notebooks\", \"models\", \"data\", \"src\", \"outputs\"]:\n","        folder_path = os.path.join(project_path, folder)\n","        os.makedirs(folder_path, exist_ok=True)\n","        gitkeep = os.path.join(folder_path, \".gitkeep\")\n","        if not os.path.exists(gitkeep):\n","            open(gitkeep, \"w\").close()\n","\n","    # Add README if not present\n","    readme_path = os.path.join(project_path, \"README.md\")\n","    if not os.path.exists(readme_path):\n","        with open(readme_path, \"w\") as f:\n","            f.write(f\"\"\"# {repo_name}\n","\n","{repo_description}\n","\n","---\n","Maintained by **{github_user}**, 2025\n","\"\"\")\n","\n","    # Initial Git push\n","    push_url = f\"https://{github_user}:{os.environ['GITHUB_TOKEN']}@github.com/{github_user}/{repo_name}.git\"\n","    !git add .\n","    !git commit -m \"Initial setup from Colab\"\n","    !git push {push_url}\n","\n","    return project_path, github_user, repo_name"],"metadata":{"id":"WVGXL7vJhnfM","executionInfo":{"status":"ok","timestamp":1753823321483,"user_tz":240,"elapsed":2,"user":{"displayName":"April Taylor","userId":"05794515634141358651"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["## RUN SETUP (RUN ONCE: Set up project structure and sync with GitHub)\n","project_path, github_user, repo_name = setup_colab_project(\n","    GITHUB_USER, REPO_NAME, USER_EMAIL, USER_FULLNAME, REPO_DESCRIPTION, DRIVE_BASE\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Buchyz48iFFu","outputId":"eaf0a6b6-e827-4439-a28c-fde143fdbe20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/ColabRepos/NLP-Qualifications-Project\n"]}]},{"cell_type":"markdown","source":["# Daily Colab Session Setup (Run Every Day)\n","üì• Mount Drive & Load Secrets\n"],"metadata":{"id":"DhW89l6UAvz-"}},{"cell_type":"code","source":["from google.colab import drive, userdata\n","import os\n","\n","# Mount your Google Drive\n","drive.mount('/content/drive')\n","\n","# Navigate to your repo\n","%cd /content/drive/MyDrive/ColabRepos/NLP-Qualifications-Project\n","\n","# Git identity (needed for commits)\n","!git config --global user.email \"Taylor.april.dawn@gmail.com\"\n","!git config --global user.name \"April Taylor\"\n","\n","# üîê Load API tokens into environment\n","os.environ['GITHUB_TOKEN'] = userdata.get('GitHubToken')\n","os.environ['WANDB_API_KEY'] = userdata.get('wandb')\n","os.environ['UMLS_API_KEY'] = userdata.get('UMLS')\n","os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n"],"metadata":{"id":"IHobC6Zi_Xj1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Log in to Weights & Biases (Optional)\n","\n","import wandb\n","wandb.login(key=os.getenv('WANDB_API_KEY'))"],"metadata":{"id":"t6SeaBeZVBw0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# END-OF-SESSION PUSH TO GITHUB\n","Run this manually after your work session to sync with GitHub"],"metadata":{"id":"2g_TnazDiqAm"}},{"cell_type":"code","source":["# Push Changes to GitHub\n","\n","!git add .\n","!git commit -m \"Daily update from Colab\" || echo \"Nothing to commit\"\n","\n","username = \"April-Taylor\"\n","token = os.getenv(\"GITHUB_TOKEN\")\n","if token:\n","    remote_url = f\"https://{username}:{token}@github.com/{username}/NLP-Qualifications-Project.git\"\n","    !git push \"{remote_url}\" main\n","else:\n","    print(\"‚ùå GitHub token not found in environment.\")\n","\n"],"metadata":{"id":"MIxOR9vtitm7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EN9--XIYPdf-"},"source":["# Load Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vq7xIDgyu4N"},"outputs":[],"source":["!pip install numpy pandas matplotlib nltk torch\n","!pip install seaborn --upgrade\n"]},{"cell_type":"code","source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import string\n","import torch\n","import re\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk_stopwords = set(nltk.corpus.stopwords.words('english'))\n","\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)"],"metadata":{"id":"f17oBYFjDuS6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d2I9iC4JMKi9"},"source":["# Data Loading"]},{"cell_type":"markdown","metadata":{"id":"MmI1k9iuPv9k"},"source":["## Load MIMIC-III curated glucose-insulin paired data"]},{"cell_type":"markdown","metadata":{"id":"M2IXwh0OOcIA"},"source":["The project uses the glucose_insulin_pair.csv dataset from the curated glucose-insulin files, available on PhysioNet (https://physionet.org/content/glucose-management-mimic/1.0.1/Notebooks/#files-panel)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qoDIupSnmehC"},"outputs":[],"source":["# Load the data into a dataframe\n","insulin_file_path = \"/content/drive/MyDrive/glucose_insulin_pair.csv\"\n","columns_needed = ['SUBJECT_ID', 'HADM_ID', 'GLC', 'GLC_AL', 'GLCSOURCE','EVENT', 'INSULINTYPE','INPUT', 'INPUT_HRS', 'INFXSTOP', 'TIMER']\n","insulin_data = pd.read_csv(insulin_file_path, usecols=columns_needed)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-d2UE6NREyHu"},"outputs":[],"source":["insulin_data.shape"]},{"cell_type":"markdown","metadata":{"id":"gvlXVp5-OLPx"},"source":["## Load MIMIC-III v1.4 notes data"]},{"cell_type":"markdown","metadata":{"id":"8Ay7QlklDgE7"},"source":["The project uses the NOTEEVENTS.csv dataset from MIMIC-III, available on PhysioNet (https://physionet.org/content/mimiciii/1.4/). These notes align with the same MIMIC-III data from which the curated glucose_insulin_pair data is derived."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DAqbD60bUvlC"},"outputs":[],"source":["# Load the data into a dataframe\n","notes_file_path = \"/content/drive/My Drive/NOTEEVENTS.csv\"\n","notes_cols = ['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'CATEGORY', 'TEXT']\n","noteevents = pd.read_csv(notes_file_path, usecols=notes_cols, low_memory=False)"]},{"cell_type":"code","source":["noteevents.shape"],"metadata":{"id":"5wwVQt7NEU2j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Path to the outputs directory\n","#some_df.to_csv(f\"{output_dir}your_filename.csv\", index=False)\n","\n","output_dir = \"/content/drive/MyDrive/ColabRepos/NLP-Qualifications-Project/outputs/\""],"metadata":{"id":"W5zqrZjv6vH6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","def save_csv(df, name, output_dir=output_dir):\n","    \"\"\"\n","    Save DataFrame to CSV in output_dir with timestamped filename.\n","    \"\"\"\n","    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n","    filename = f\"{output_dir}{name}_{timestamp}.csv\"\n","    df.to_csv(filename, index=False)\n","    print(f\"‚úÖ Saved: {filename}\")\n"],"metadata":{"id":"KuIuvaMg65qm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inspect and Preprocess Structured Data (Insulin-Glucose Pairs)"],"metadata":{"id":"WJHaXzB9kLQf"}},{"cell_type":"markdown","metadata":{"id":"STxMHL38ANYP"},"source":["## Inspect the Glucose_Insulin_Pair dataset"]},{"cell_type":"markdown","metadata":{"id":"bD8RgSnfEWzQ"},"source":["**Description of fields:**\n","*   **SUBJECT_ID:** It is the unique identifier for an individual patient.\n","*  **HADM_ID:**Represents a single patient‚Äôs admission to the hospital.\n","*   **ICUSTAY_ID:** Unique identifier for a single patient‚Äôs admission to the ICU.\n","*   **LOS_ICU_days:** Length of stay in days.\n","*   **first_ICU_stay:** True if it is the first admission to the ICU for a hospital admission.\n","*   **TIMER:** Gathers the timestamps for either the STARTTIME for a single insulin input or the GLCTIMER for a single glucose reading. It is used to order chronologically the events along a hospital admission.\n","*   **STARTTIME:** Timestamp that depicts when the administration of an insulin event started or when a new infusion rate was indicated.\n","*   **INPUT:** Dose for a single bolus of insulin in U.\n","*   **INPUT_HRS:** Insulin infusion rate in units/hr.\n","*   **ENDTIME:** Timestamp that specifies when an insulin input stopped, or an infusion rate changed.\n","*   **INSULINTYPE:** Acting type of insulin: short, intermediate, or long.\n","*   **EVENT:** Specifies whether the bolus of insulin was subcutaneous (BOLUS_INYECTION), or intravenous (BOLUS_PUSH), or if the insulin was infused (INFUSION).\n","*   **INFXSTOP:** Indicates when an infusion of insulin was discontinued. A value equal to 1 indicates when an infusion was discontinued, otherwise (entries equal to 0) this column indicates that the associated infusion started or the rate of infusion was modified.\n","*   **GLCTIMER:** Timestamp that depicts when a glycemic check was done.\n","*   **GLC:** Glycemia value in mg/dL.\n","*   **GLCSOURCE:** Reading method for a glycemic check: fingerstick (FINGERSTICK) or lab analyzer (BLOOD).\n","\n","Specific to glucose_insulin_pair.csv\n","*   **GLCTIMER_AL:** Timestamp that depicts when a glycemic check was done for a paired glucose reading. This value should match with the timestamp in GLCTIMER of a preceding glucose reading according to the rule applied for this pairing case.\n","*   **GLC_AL:** Glycemia value in mg/dL for a paired glucose reading with a single insulin input. This value should match with the value in GLC of a preceding glucose reading according to the rule applied for this pairing case.\n","*   **GLCSOURCE_AL:** Reading method for a glycemic check that was paired with an insulin input. This value should match with the GLCSOURCE value of a preceding glucose reading according to the rule applied for this pairing case.\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zgTZjQb6nM_O"},"outputs":[],"source":["# Inspect the Glucose_Insulin_Pair dataset\n","print(insulin_data.head(10))\n","print(insulin_data.info())\n","print(insulin_data.describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5SvSFDzhqtb"},"outputs":[],"source":["#Check for Missing Times in Insulin Data\n","print(f\"Missing TIMER in insulin_data: {insulin_data['TIMER'].isna().sum()} out of {len(insulin_data)}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nMZ0kIYwrqD4"},"source":["## PreProcess Glucose_Insulin Data"]},{"cell_type":"markdown","metadata":{"id":"DWP84ZoY3tOa"},"source":["### Filter and format glucose/insulin data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5oyWGWUMpX6z"},"outputs":[],"source":["# Convert HADM_ID to int\n","insulin_data['HADM_ID'] = insulin_data['HADM_ID'].fillna(-1).astype(int)\n","\n","# Ensure TIMER is datetime and timezone-naive for both insulin and glucose datasets\n","insulin_data['TIMER'] = pd.to_datetime(insulin_data['TIMER'], errors='coerce').dt.tz_localize(None)\n","if insulin_data['TIMER'].isna().any():\n","    print(\"Invalid TIMER values detected.\")\n","\n","\n","# Verify that all TIMER values are properly formatted\n","assert insulin_data['TIMER'].notna().all(), \"TIMER in insulin_data contains NaT!\"\n","\n","\n","# Remove fully duplicate rows (all columns identical)\n","insulin_data = insulin_data.drop_duplicates()\n","\n","# Sort the insulin_data by TIMER for merging\n","insulin_data = insulin_data.sort_values(by='TIMER').reset_index(drop=True)\n","\n","# Verify sorting\n","assert insulin_data['TIMER'].is_monotonic_increasing, \"TIMER column is not sorted!\"\n","\n","# Final check of the data structure\n","print(\"Final insulin_data structure:\")\n","print(insulin_data.info())\n","\n","# Sample to verify the structure\n","print(\"Sample of data after removing duplicates:\")\n","print(insulin_data.head())\n"]},{"cell_type":"markdown","metadata":{"id":"jZB3fPsZfeVZ"},"source":["### Create a combined column for all glucose values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNRRMwq9fc2v"},"outputs":[],"source":["# Separate rows with GLC_AL values\n","glc_al_data = insulin_data[~insulin_data['GLC_AL'].isna()].copy()\n","\n","# Separate rows with GLC values\n","glc_only_data = insulin_data[~insulin_data['GLC'].isna()].copy()\n","\n","\n","\n","# Sort both datasets by TIMER\n","glc_only_data = glc_only_data.sort_values('TIMER')\n","glc_al_data = glc_al_data.sort_values('TIMER')\n","\n","# Create a temporary merge to find matches within the 2-hour window\n","merged_within_2hr = pd.merge_asof(\n","    glc_only_data,\n","    glc_al_data[['TIMER']],  # Only use TIMER column from glc_al_data for matching\n","    on='TIMER',\n","    tolerance=pd.Timedelta(hours=2),  # Define the 2-hour window\n","    direction='nearest'  # Allow matching in both directions\n",")\n","\n","# Reset index of glc_only_data and merged_within_2hr before filtering\n","glc_only_data = glc_only_data.reset_index(drop=True)\n","merged_within_2hr = merged_within_2hr.reset_index(drop=True)\n","\n","\n","# Filter out rows from glc_only_data that have a match in glc_al_data within 2 hours\n","filtered_glc_only_data = glc_only_data[merged_within_2hr['TIMER'].isna()]\n","\n","# Combine GLC_AL and filtered GLC data\n","GLC_ALL = pd.concat([glc_al_data, filtered_glc_only_data], ignore_index=True)\n","\n","\n","# Add GLC_ALL column to insulin_data for consistency\n","insulin_data['GLC_ALL'] = insulin_data['GLC_AL'].combine_first(insulin_data['GLC'])\n","\n","# Verify the structure of insulin_data\n","print(\"Updated insulin_data with GLC_ALL:\")\n","print(insulin_data.info())\n","print(insulin_data[['TIMER', 'GLC', 'GLC_AL', 'GLC_ALL']].head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cn3Zf_WmUVK"},"outputs":[],"source":["# Aggregate insulin_data at SUBJECT_ID, HADM_ID level\n","insulin_aggregated = insulin_data.groupby(['SUBJECT_ID', 'HADM_ID']).agg(\n","    TotalBolus=('INPUT', 'sum'),                  # Total bolus insulin administered\n","    AvgBolus=('INPUT', 'mean'),                  # Average bolus insulin per event\n","    MaxBolus=('INPUT', 'max'),                   # Max bolus insulin\n","    TotalInfusion=('INPUT_HRS', 'sum'),          # Total infusion hours\n","    AvgInfusionRate=('INPUT_HRS', 'mean'),       # Average infusion rate\n","    MaxGlucose=('GLC_ALL', 'max'),                   # Maximum glucose level\n","    MinGlucose=('GLC_ALL', 'min'),                   # Minimum glucose level\n","    AvgGlucose=('GLC_ALL', 'mean'),                  # Average glucose level\n","    InfusionStops=('INFXSTOP', 'sum')            # Count of infusion stops\n",").reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCeX7GmFocl_"},"outputs":[],"source":["print(insulin_aggregated.head())"]},{"cell_type":"markdown","metadata":{"id":"z5rP3iRL8Vfk"},"source":["## Categorize Glucose_Insulin data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6llLS2tvZvI"},"outputs":[],"source":["# Categorize insulin delivery types\n","def categorize_insulin_delivery(row):\n","    if pd.isna(row['INPUT']):  # No bolus given\n","        if pd.isna(row['INPUT_HRS']):  # No infusion rate recorded\n","            return \"No insulin adjustment\"\n","    elif isinstance(row['EVENT'], str) and (\"BOLUS_INYECTION\" in row['EVENT'] or \"BOLUS_PUSH\" in row['EVENT']):\n","        return \"Bolus\"\n","    elif isinstance(row['EVENT'], str) and \"INFUSION\" in row['EVENT']:\n","        return \"Infusion\"\n","    return \"Unknown\"  # Fallback case\n","\n","# Apply the function to categorize insulin delivery\n","insulin_data['InsulinDeliveryCategory'] = insulin_data.apply(categorize_insulin_delivery, axis=1)\n","\n","# Categorize glucose levels\n","def categorize_glucose(value):\n","    if pd.isna(value):  # Handle NaN case\n","        return None\n","    elif value < 70:\n","        return \"Hypoglycemic\"\n","    elif value > 180:\n","        return \"Hyperglycemic\"\n","    else:\n","        return \"Euglycemic\"\n","\n","# Apply glucose categorization to the GLC_ALL column\n","insulin_data['GlucoseCategory'] = insulin_data['GLC_ALL'].apply(categorize_glucose)\n","\n","# Verify updated categories\n","print(\"\\nUnique Insulin Delivery Categories in categorized_data after refined processing:\")\n","print(insulin_data['InsulinDeliveryCategory'].unique())\n","\n","print(\"\\nUnique Glucose Categories in insulin_data after refined processing:\")\n","print(insulin_data['GlucoseCategory'].unique())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHGFJWoi77e_"},"outputs":[],"source":["# Count the number of glucose events by category\n","glucose_category_counts = insulin_data['GlucoseCategory'].value_counts()\n","\n","# Bar Plot for Glucose Category Distribution with AGP Colors\n","plt.figure(figsize=(10, 6))\n","\n","# Define AGP colors for each category\n","agp_colors = {\n","    'Hypoglycemic': 'red',\n","    'Euglycemic': 'green',\n","    'Hyperglycemic': 'orange'\n","}\n","\n","# Get the colors for the bars in the correct order\n","bar_colors = [agp_colors[category] for category in glucose_category_counts.index]\n","\n","# Create the bar plot with AGP colors\n","bars = sns.barplot(\n","    x=glucose_category_counts.index,\n","    y=glucose_category_counts.values,\n","    palette=bar_colors  # Use the AGP colors\n",")\n","\n","plt.title('Distribution of Glucose Categories')\n","plt.xlabel('Glucose Category')\n","plt.ylabel('Count')\n","plt.xticks(rotation=45)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Asp9ZO4qNGpL"},"outputs":[],"source":["# Count occurrences of each label in 'GlucoseCategory'\n","glucose_category_counts = insulin_data['GlucoseCategory'].value_counts()\n","print(\"Glucose Category Counts:\\n\", glucose_category_counts)\n","\n","# Count occurrences of each label in 'InsulinDeliveryCategory'\n","insulin_delivery_counts = insulin_data['InsulinDeliveryCategory'].value_counts()\n","print(\"\\nInsulin Delivery Category Counts:\\n\", insulin_delivery_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QdsmgevJ-Z66"},"outputs":[],"source":["# Frequency of insulin categories\n","plt.figure(figsize=(10, 6))\n","sns.countplot(data=insulin_data, x='InsulinDeliveryCategory', order=insulin_data['InsulinDeliveryCategory'].value_counts().index)\n","plt.title('Frequency of Insulin Delivery Categories')\n","plt.xlabel('Insulin Delivery Category')\n","plt.ylabel('Count')\n","plt.xticks(rotation=45)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"qA50KWZsgDo2"},"source":["# Exploratory Data Analysis (EDA)"]},{"cell_type":"markdown","metadata":{"id":"B8CVZuDWAgt7"},"source":["##Distributions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWb0zjKiJ_rX"},"outputs":[],"source":["# Statistical summaries for Glucose, Bolus, and Infusion\n","summary_stats = insulin_data[['GLC_ALL', 'INPUT', 'INPUT_HRS']].rename(columns={\n","    'GLC_ALL': 'Glucose',\n","    'INPUT': 'Bolus',\n","    'INPUT_HRS': 'Infusion'\n","}).describe()\n","\n","print(\"Statistical Summary:\")\n","print(summary_stats)\n"]},{"cell_type":"code","source":["# Convert the summary statistics to a DataFrame for better formatting\n","summary_stats_df = summary_stats.T\n","\n","# Display the DataFrame as a formatted table\n","summary_stats_df\n","\n","# Export to CSV\n","summary_stats_df.to_csv('summary_stats.csv', index=True)"],"metadata":{"id":"CIOQy-g6t5bG"},"execution_count":null,"outputs":[]},{"source":["# Consolidated Distribution Plot\n","def plot_distribution(data, column, title, xlabel, bins=30, color='blue'):\n","    plt.figure(figsize=(10, 6))\n","    sns.histplot(data[column], bins=bins, kde=True, color=color)\n","    plt.title(title)\n","    plt.xlabel(xlabel)\n","    plt.ylabel('Frequency')\n","\n","# Plot Glucose Levels\n","plot_distribution(insulin_data, 'GLC_ALL', 'Distribution of Glucose Levels', 'Glucose (mg/dL)', color='red')\n","plt.xticks(range(0, int(insulin_data['GLC_ALL'].max()) + 100, 100))\n","plt.show()\n","\n","# Plot Bolus Insulin Doses\n","plot_distribution(insulin_data, 'INPUT', 'Distribution of Bolus Insulin Doses', 'Bolus Insulin (units)', color='orange')\n","plt.show()\n","\n","# Plot Infusion Rates\n","plot_distribution(insulin_data, 'INPUT_HRS', 'Distribution of Infusion Rates', 'Infusion Rate (units/hr)', color='blue')\n","plt.show()"],"cell_type":"code","metadata":{"id":"RN3leYlNk00z"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"svIT8S50BdYQ"},"outputs":[],"source":["# Select relevant columns for plotting\n","boxplot_data = insulin_data[['GLC_ALL', 'INPUT', 'INPUT_HRS']].rename(columns={\n","    'GLC_ALL': 'Glucose',\n","    'INPUT': 'Bolus',\n","    'INPUT_HRS': 'Infusion'\n","})\n","\n","# Define the column names and titles for the plots\n","columns = ['Glucose', 'Bolus', 'Infusion']\n","titles = ['Distribution of Glucose Levels', 'Distribution of Bolus Insulin Doses', 'Distribution of Infusion Rates']\n","\n","# Create individual boxplots\n","for i, col in enumerate(columns):\n","    plt.figure(figsize=(8, 6))\n","    sns.boxplot(data=boxplot_data, y=col, color='skyblue', width=0.5)\n","\n","    # Add plot labels and title\n","    plt.title(titles[i], fontsize=16)\n","    plt.ylabel(col, fontsize=14)\n","    plt.xticks([])  # Remove x-ticks for simplicity\n","    plt.grid(axis='y', linestyle='--', alpha=0.7)\n","\n","    # Adjust y-axis ticks for better granularity\n","    if col == 'Bolus':  # For Bolus, set y-ticks to every 20 units\n","        plt.yticks(range(0, int(boxplot_data[col].max() + 1), 20))\n","    elif col != 'Glucose':  # For Infusion, finer granularity (1 unit steps)\n","        plt.yticks(range(0, int(boxplot_data[col].max() + 1), 1))\n","\n","    # Display the plot\n","    plt.tight_layout()\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFlK27E60Qb_"},"outputs":[],"source":["hypoglycemic_cases = insulin_data[insulin_data['GLC_ALL'] < 70]\n","print(hypoglycemic_cases)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tkX9SAZd_oig"},"outputs":[],"source":["# Count of each insulin category\n","insulin_category_counts = insulin_data['InsulinDeliveryCategory'].value_counts()\n","print(\"Insulin Category Counts:\\n\", insulin_category_counts)\n","\n","# Count of each glucose category\n","glucose_category_counts = insulin_data['GlucoseCategory'].value_counts()\n","print(\"\\nGlucose Category Counts:\\n\", glucose_category_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmmi3RTNhgAd"},"outputs":[],"source":["# Group data by GlucoseCategory, InsulinDeliveryCategory, and INFXSTOP\n","grouped_data = insulin_data.groupby(['GlucoseCategory', 'InsulinDeliveryCategory']).size().reset_index(name='Count')\n","\n","# Pivot table for heatmap (optional to include INFXSTOP as additional dimension)\n","pivot_data = grouped_data.pivot_table(index='GlucoseCategory', columns='InsulinDeliveryCategory', values='Count', aggfunc='sum')\n","\n","plt.figure(figsize=(10, 6))\n","sns.heatmap(\n","    pivot_data,\n","    annot=True,\n","    fmt=\".0f\",\n","    cmap=\"YlGnBu\",\n","    linewidths=0.5,\n","    cbar_kws={\"label\": \"Event Count\"}\n",")\n","plt.title('Insulin Delivery by Glucose Category')\n","plt.xlabel('Insulin Delivery Method')\n","plt.ylabel('Glucose Category')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RyhgcLNVuU47"},"outputs":[],"source":["# Group by SUBJECT_ID and calculate metrics\n","grouped_data = insulin_data.groupby('SUBJECT_ID').agg({\n","    'GLC_ALL': 'mean',  # Average glucose level\n","    'INPUT': 'mean',  # Average insulin dose per event\n","    'GlucoseCategory': 'count'  # Number of glucose readings\n","}).rename(columns={'GLC_ALL': 'AvgGlucose', 'INPUT': 'AvgInsulinPerEvent', 'GlucoseCategory': 'NumReadings'})\n","\n","# Full scatterplot\n","plt.figure(figsize=(12, 8))\n","sns.scatterplot(\n","    data=grouped_data,\n","    x='AvgInsulinPerEvent',\n","    y='AvgGlucose',\n","    hue='NumReadings',\n","    palette='coolwarm',\n","    size='NumReadings',\n","    sizes=(20, 200)\n",")\n","plt.axhline(y=70, color='red', linestyle='--', label='Hypoglycemia (<70 mg/dL)')\n","plt.axhline(y=180, color='orange', linestyle='--', label='Hyperglycemia (>180 mg/dL)')\n","plt.title('Relationship between Avg. Insulin Dose per Event and Avg. Glucose Levels (Full Range)', fontsize=16)\n","plt.xlabel('Avg. Insulin Dose per Event (units)', fontsize=14)\n","plt.ylabel('Average Glucose Level (mg/dL)', fontsize=14)\n","plt.legend(title=\"Num. of Readings\", loc='upper right', fontsize=10)\n","plt.grid(True)\n","plt.show()\n","\n","# Filter for granular zoom: 1‚Äì20 units of insulin\n","filtered_zoom_data = grouped_data[(grouped_data['AvgInsulinPerEvent'] >= 1) & (grouped_data['AvgInsulinPerEvent'] <= 20)]\n","\n","# Zoomed scatterplot\n","plt.figure(figsize=(12, 8))\n","sns.scatterplot(\n","    data=filtered_zoom_data,\n","    x='AvgInsulinPerEvent',\n","    y='AvgGlucose',\n","    hue='NumReadings',\n","    palette='coolwarm',\n","    size='NumReadings',\n","    sizes=(20, 200)\n",")\n","plt.axhline(y=70, color='red', linestyle='--', label='Hypoglycemia (<70 mg/dL)')\n","plt.axhline(y=180, color='orange', linestyle='--', label='Hyperglycemia (>180 mg/dL)')\n","plt.title('Relationship between Avg. Insulin Dose per Event and Avg. Glucose Levels (1‚Äì20 Units)', fontsize=16)\n","plt.xlabel('Avg. Insulin Dose per Event (units)', fontsize=14)\n","plt.ylabel('Average Glucose Level (mg/dL)', fontsize=14)\n","plt.xticks(ticks=range(1, 21, 1))  # Sets x-axis ticks from 1 to 20 with step of 1\n","plt.legend(title=\"Num. of Readings\", loc='upper right', fontsize=10)\n","plt.grid(True)\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"a2xx2gBtBMn_"},"source":["##Time Series Comparisons"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVijGTz1BQtH"},"outputs":[],"source":["import random\n","import matplotlib.pyplot as plt\n","\n","random.seed(42)\n","\n","def plot_time_series(insulin_data, subject_ids, random_seed=42):\n","    \"\"\"Plots time series for specified subjects, highlighting insulin values with black labels and red dots.\"\"\"\n","\n","    for subject_id in subject_ids:\n","        subject_data = insulin_data[insulin_data['SUBJECT_ID'] == subject_id]\n","        if not subject_data.empty:\n","            plt.figure(figsize=(15, 4))\n","\n","            # Plot glucose levels\n","            plt.plot(subject_data['TIMER'], subject_data['GLC_ALL'], label='Glucose (mg/dL)', color='blue')\n","\n","            # Highlight insulin data points as red dots\n","            insulin_data_points = subject_data[~subject_data['INPUT'].isna()]\n","            plt.scatter(\n","                insulin_data_points['TIMER'],\n","                insulin_data_points['INPUT'],\n","                color='red',\n","                label='Bolus Doses (units)',\n","                marker='o',\n","                s=50\n","            )\n","\n","            # Add data labels for insulin points (as whole numbers and black text)\n","            for _, row in insulin_data_points.iterrows():\n","                plt.text(\n","                    row['TIMER'],\n","                    row['INPUT'] + 10,  # Offset the label slightly above the point\n","                    f\"{int(row['INPUT'])}\",  # Convert to whole number\n","                    fontsize=9,\n","                    color='black',\n","                    ha='center'\n","                )\n","\n","            plt.xlabel('Date')\n","            plt.ylabel('Values')\n","            plt.title(f'Time Series for Subject {subject_id}')\n","            plt.xticks(rotation=45)\n","            plt.legend()\n","            plt.tight_layout()\n","            plt.ylim(0, 400)\n","            plt.show()\n","\n","# Get three random subject IDs from the data\n","num_subjects_to_plot = 3\n","subject_ids = random.sample(list(insulin_data['SUBJECT_ID'].unique()), num_subjects_to_plot)\n","\n","# Plot the time series for those subjects, one plot per subject\n","plot_time_series(insulin_data, subject_ids, random_seed=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0NYP9MLz_16L"},"outputs":[],"source":["# Inspect raw insulin and glucose values for the selected patient\n","patient_id =55642  # Replace with the patient ID from your plot\n","raw_patient_data = insulin_data[insulin_data['SUBJECT_ID'] == patient_id]\n","\n","# Display raw insulin and glucose data\n","print(\"Raw Data for Patient:\")\n","print(raw_patient_data[['TIMER', 'GLC_ALL', 'INPUT', 'INPUT_HRS', 'InsulinDeliveryCategory']])\n","\n","# Check unique values for INPUT and INPUT_HRS\n","print(\"\\nUnique Bolus Insulin Doses (INPUT):\")\n","print(raw_patient_data['INPUT'].unique())\n","\n","print(\"\\nUnique Infusion Rates (INPUT_HRS):\")\n","print(raw_patient_data['INPUT_HRS'].unique())\n","\n","# Check if there are duplicates or overlapping data\n","duplicates = raw_patient_data.duplicated(subset=['TIMER', 'INPUT', 'INPUT_HRS'])\n","print(f\"\\nNumber of Duplicate Entries for Patient {patient_id}: {duplicates.sum()}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RXm1LFkF_as"},"outputs":[],"source":["!pip install adjustText\n","from adjustText import adjust_text\n","\n","\n","# Ensure TIMER is a datetime object\n","insulin_data['TIMER'] = pd.to_datetime(insulin_data['TIMER'], errors='coerce')\n","\n","# Extract Hour and Day from TIMER\n","insulin_data['Hour'] = insulin_data['TIMER'].dt.hour\n","insulin_data['Day'] = insulin_data['TIMER'].dt.date\n","\n","# Group by Hour and calculate mean values for Glucose, Bolus Insulin, and Infusion Rate\n","hourly_avg = insulin_data.groupby('Hour').agg({\n","    'GLC_ALL': 'mean',  # Average Glucose Level\n","    'INPUT': 'mean',         # Average Bolus Insulin\n","    'INPUT_HRS': 'mean'      # Average Infusion Rate\n","}).rename(columns={\n","    'GLC_ALL': 'Glucose (mg/dL)',\n","    'INPUT': 'Bolus (units)',\n","    'INPUT_HRS': 'Infusion (units/hr)'\n","}).reset_index()\n","\n","# Plot the hourly trends\n","plt.figure(figsize=(14, 8))\n","\n","# Initialize the list for dynamic text annotations\n","texts = []\n","\n","# Plot Glucose Levels\n","plt.plot(hourly_avg['Hour'], hourly_avg['Glucose (mg/dL)'], marker='o', label='Glucose (mg/dL)', color='blue', linestyle='-')\n","for _, row in hourly_avg.iterrows():\n","    texts.append(\n","        plt.text(\n","            row['Hour'], row['Glucose (mg/dL)'],  # Add text dynamically\n","            f\"{round(row['Glucose (mg/dL)'], 1)}\",\n","            fontsize=9, color='blue', ha='center'\n","        )\n","    )\n","\n","# Plot Bolus Insulin Levels\n","plt.plot(hourly_avg['Hour'], hourly_avg['Bolus (units)'], marker='x', label='Bolus (units)', color='red', linestyle='--')\n","for _, row in hourly_avg.iterrows():\n","    texts.append(\n","        plt.text(\n","            row['Hour'], row['Bolus (units)'],  # Add text dynamically\n","            f\"{round(row['Bolus (units)'], 1)}\",\n","            fontsize=9, color='red', ha='center'\n","        )\n","    )\n","\n","# Plot Infusion Rates\n","plt.plot(hourly_avg['Hour'], hourly_avg['Infusion (units/hr)'], marker='s', label='Infusion (units/hr)', color='green', linestyle='-.')\n","for _, row in hourly_avg.iterrows():\n","    texts.append(\n","        plt.text(\n","            row['Hour'], row['Infusion (units/hr)'],  # Add text dynamically\n","            f\"{round(row['Infusion (units/hr)'], 1)}\",\n","            fontsize=9, color='green', ha='center'\n","        )\n","    )\n","\n","# Adjust overlapping text labels\n","adjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray'))\n","\n","# Customize plot\n","plt.title('Hourly Trends in Glucose and Insulin Levels (1-24 Hours)', fontsize=16)\n","plt.xlabel('Hour of Day (0-23)', fontsize=14)\n","plt.ylabel('Average Levels', fontsize=14)\n","\n","# Set y-axis ticks and limits\n","y_min = 0  # Minimum value for y-axis\n","y_max = int(hourly_avg[['Glucose (mg/dL)', 'Bolus (units)', 'Infusion (units/hr)']].max().max() + 20)\n","plt.yticks(np.arange(y_min, y_max + 1, 20))  # Intervals of 20\n","\n","# Configure x-axis ticks for hourly scale\n","plt.xticks(ticks=np.arange(0, 24), labels=np.arange(0, 24), fontsize=12)\n","plt.grid(True, linestyle='--', alpha=0.6)\n","\n","# Adjust legend placement\n","plt.legend(title=\"Legend\", fontsize=12, loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n","\n","# Adjust layout to accommodate legend\n","plt.tight_layout(rect=[0, 0.05, 1, 1])\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2BlA8wAcc2J"},"outputs":[],"source":["\n","\n","# Filter for patients with both insulin and glucose values\n","pairwise_distances_argmin_min_data = insulin_data[\n","    (insulin_data['GLC_ALL'].notna()) &  # Glucose value present\n","    (\n","       insulin_data['INPUT'].notna() |  # Bolus insulin\n","        insulin_data['INPUT_HRS'].notna() |  # Infusion rate\n","        (insulin_data['INFXSTOP'] == 1)  # Infusion stop\n","    )\n","]\n","\n","# Get a list of unique patient IDs from paired data\n","valid_patient_ids = insulin_data['SUBJECT_ID'].unique()\n","\n","# Select 3 random patient IDs from valid ones\n","if len(valid_patient_ids) < 3:\n","    print(\"Not enough patients with both insulin and glucose values for plotting.\")\n","else:\n","    random_patient_ids = random.sample(list(valid_patient_ids), 3)\n","\n","    # Iterate over each random patient ID and create individual plots\n","    for patient_id in random_patient_ids:\n","        # Filter data for this patient\n","        patient_data = insulin_data[insulin_data['SUBJECT_ID'] == patient_id].copy()\n","\n","        # Ensure TIMER is sorted\n","        patient_data = patient_data.sort_values(by='TIMER')\n","\n","        # Check if patient data is empty after filtering\n","        if patient_data.empty:\n","            print(f\"No valid data for patient ID {patient_id}.\")\n","            continue\n","\n","        # Create text annotations list\n","        texts = []\n","\n","        # Initialize the figure\n","        plt.figure(figsize=(14, 8))\n","\n","        # Plot Glucose Levels\n","        glucose_data = patient_data[patient_data['GLC_ALL'] > 0]\n","        plt.plot(\n","            glucose_data['TIMER'],\n","            glucose_data['GLC_ALL'],\n","            color='blue',\n","            label='Glucose Levels (mg/dL)',\n","            marker='o',\n","            linestyle='-',\n","            alpha=0.7\n","        )\n","        for _, row in glucose_data.iterrows():\n","            texts.append(\n","                plt.text(\n","                    row['TIMER'], row['GLC_ALL'] + 5,  # Offset upward\n","                    f\"{int(row['GLC_ALL'])} mg/dL\",\n","                    fontsize=10,\n","                    color='blue',\n","                    ha='center'\n","                )\n","            )\n","\n","        # Scatterplot for Bolus Insulin\n","        bolus_data = patient_data[patient_data['INPUT'] > 0]\n","        plt.scatter(\n","            bolus_data['TIMER'],\n","            bolus_data['INPUT'],\n","            color='red',\n","            label='Bolus Insulin (Units)',\n","            alpha=0.7,\n","            marker='x'\n","        )\n","        for _, row in bolus_data.iterrows():\n","            texts.append(\n","                plt.text(\n","                    row['TIMER'], row['INPUT'] + 5,  # Offset upward\n","                    f\"{int(row['INPUT'])} units\",\n","                    fontsize=10,\n","                    color='red',\n","                    ha='center'\n","                )\n","            )\n","\n","        # Scatterplot for Infusion Rates\n","        infusion_data = patient_data[patient_data['INPUT_HRS'] > 0]\n","        plt.scatter(\n","            infusion_data['TIMER'],\n","            infusion_data['INPUT_HRS'],\n","            color='green',\n","            label='Infusion Rate (Units/hr)',\n","            alpha=0.7,\n","            marker='s'\n","        )\n","        for _, row in infusion_data.iterrows():\n","            texts.append(\n","                plt.text(\n","                    row['TIMER'], row['INPUT_HRS'] + 5,  # Offset upward\n","                    f\"{int(row['INPUT_HRS'])} units/hr\",\n","                    fontsize=10,\n","                    color='green',\n","                    ha='center'\n","                )\n","            )\n","\n","        # Scatterplot for Infusion Stops\n","        infusion_stop_data = patient_data[patient_data['INFXSTOP'] == 1]\n","        plt.scatter(\n","            infusion_stop_data['TIMER'],\n","            infusion_stop_data['INPUT_HRS'],\n","            color='purple',\n","            label='Infusion Stopped',\n","            alpha=0.9,\n","            marker='P',\n","            s=150\n","        )\n","        for _, row in infusion_stop_data.iterrows():\n","            texts.append(\n","                plt.text(\n","                    row['TIMER'], row['INPUT_HRS'] + 5,  # Offset upward\n","                    \"Stopped\",\n","                    fontsize=10,\n","                    color='purple',\n","                    ha='center'\n","                )\n","            )\n","\n","        # Adjust overlapping text labels\n","        adjust_text(texts, arrowprops=dict(arrowstyle='-', color='black'))\n","\n","        # Customize the plot\n","        plt.title(f'Overlay of Glucose and Insulin Trends for Patient ID {patient_id}', fontsize=16)\n","        plt.xlabel('Time', fontsize=14)\n","        plt.ylabel('Values (Glucose Levels and Insulin Doses)', fontsize=14)\n","        plt.legend(fontsize=12)\n","        plt.ylim(0)\n","        plt.grid(True)\n","        plt.xticks(rotation=45)\n","\n","        # Show the plot\n","        plt.tight_layout()\n","        plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"HYIRkJJnAt9U"},"source":["##Correlations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QayTMDyaKagW"},"outputs":[],"source":["# Correlation matrix\n","correlation_matrix = insulin_data[['GLC_ALL', 'INPUT', 'INPUT_HRS']].rename(columns={\n","    'GLC_ALL': 'Glucose',\n","    'INPUT': 'Bolus',\n","    'INPUT_HRS': 'Infusion'\n","}).corr(method='pearson')\n","\n","print(\"Correlation Matrix:\")\n","print(correlation_matrix)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-tk5PG3lIUcT"},"outputs":[],"source":["# Select relevant columns for correlation analysis\n","correlation_data = insulin_data[['GLC_ALL', 'INPUT', 'INPUT_HRS']].rename(columns={\n","    'GLC_ALL': 'Glucose',\n","    'INPUT': 'Bolus',\n","    'INPUT_HRS': 'Infusion'\n","})\n","\n","# Calculate the correlation matrix\n","correlation_matrix = correlation_data.corr()\n","\n","# Plot the correlation heatmap\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(\n","    correlation_matrix,\n","    annot=True,  # Display correlation coefficients\n","    cmap='coolwarm',  # Color map for visualization\n","    fmt=\".2f\",  # Format the correlation coefficients\n","    linewidths=0.5,  # Add space between cells\n","    cbar_kws={\"shrink\": 0.8}  # Shrink color bar for better layout\n",")\n","\n","# Customize plot labels and title\n","plt.title('Correlation Matrix: Glucose, Bolus, and Infusion', fontsize=16)\n","plt.xticks(rotation=45, fontsize=12)\n","plt.yticks(rotation=45, fontsize=12)\n","\n","# Display the plot\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imFj45WFKi31"},"outputs":[],"source":["# Import the required function\n","from scipy.stats import ttest_ind\n","\n","# Separate groups\n","infusion_group = insulin_data[insulin_data['INPUT_HRS'] > 0]['GLC_ALL'].dropna()  # Glucose levels for patients on infusion\n","bolus_group = insulin_data[insulin_data['INPUT'] > 0]['GLC_ALL'].dropna()  # Glucose levels for patients on bolus\n","\n","# Check if either group is empty after removing NaNs\n","if len(infusion_group) == 0 or len(bolus_group) == 0:\n","    print(\"One or both groups are empty after removing NaN values. Cannot perform t-test.\")\n","else:\n","    # Perform t-test\n","    t_stat, p_value = ttest_ind(infusion_group, bolus_group, equal_var=False)\n","\n","    # Print results\n","    print(f\"T-Test Results: t-statistic = {t_stat}, p-value = {p_value}\")\n","\n","# Interpret results\n","if p_value < 0.05:\n","    print(\"There is a significant difference in glucose levels between patients on infusion vs. bolus insulin.\")\n","else:\n","    print(\"No significant difference in glucose levels between patients on infusion vs. bolus insulin.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1nOB2g6LOo-"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split # Import train_test_split\n","# Prepare the data\n","# Map categories to numerical values, handling potential NaN values\n","insulin_data['GlucoseCategoryEncoded'] = insulin_data['GlucoseCategory'].map({\n","    'Hypoglycemic': 0,\n","    'Euglycemic': 1,\n","    'Hyperglycemic': 2\n","})\n","\n","# Select features and target\n","features = ['INPUT', 'INPUT_HRS']  # Bolus and infusion insulin rates\n","target = 'GlucoseCategoryEncoded'\n","\n","X = insulin_data[features]\n","y = insulin_data[target]\n","\n","# Impute NaN values with the mean of each column using SimpleImputer\n","from sklearn.impute import SimpleImputer\n","imputer = SimpleImputer(strategy='mean') # You can also use 'median' or other strategies\n","X = imputer.fit_transform(X)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","\n","# Drop rows with NaN values in the target variable 'y'\n","# Create a boolean mask indicating rows with NaN in 'y'\n","nan_mask = y.isna()\n","\n","# Filter out rows with NaN in both X and y\n","X_scaled = X_scaled[~nan_mask]\n","y = y[~nan_mask]\n","\n","# Split data into train and test sets (after dropping NaNs)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# Check class distribution in y_train\n","print(\"Class distribution in training data:\", np.bincount(y_train.astype(int)))  # Convert to int for bincount\n","\n","# Ensure all classes are present in training data\n","classes = np.unique(np.concatenate((y_train, y_test)))  # Get all unique classes from both train and test\n","\n","# Calculate class weights to address class imbalance\n","# Assuming 'balanced' strategy, adjust as needed\n","from sklearn.utils.class_weight import compute_class_weight\n","class_weight_dict = dict(enumerate(compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n","\n","\n","# Train the logistic regression model\n","from sklearn.linear_model import LogisticRegression # Import LogisticRegression\n","model = LogisticRegression(\n","    multi_class='multinomial', solver='lbfgs', max_iter=1000, class_weight=class_weight_dict\n",")\n","model.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = model.predict(X_test)\n","\n","# Classification report with explicit labels\n","print(\"Classification Report:\")\n","from sklearn.metrics import classification_report # Import classification_report\n","print(classification_report(\n","    y_test, y_pred,\n","    labels=[0, 1, 2],\n","    target_names=['Hypoglycemic', 'Euglycemic', 'Hyperglycemic']\n","))\n","\n","# Confusion matrix\n","from sklearn.metrics import confusion_matrix # Import confusion_matrix\n","conf_matrix = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(\n","    conf_matrix,\n","    annot=True,\n","    fmt='d',\n","    cmap='Blues',\n","    xticklabels=['Hypoglycemic', 'Euglycemic', 'Hyperglycemic'],\n","    yticklabels=['Hypoglycemic', 'Euglycemic', 'Hyperglycemic']\n",")\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"yVu0iiDtkfZw"},"source":["# Inspect and Filter Unstructured Data (Noteevents dataset)\n","(Will do deeper preprocessing after UMLS linking)"]},{"cell_type":"markdown","metadata":{"id":"hqyGi7YmHBFx"},"source":["**Description of fields:**\n","\n","\n","*   **ROW_ID:** Unique identifier for each row in the dataset.\n","*   **SUBJECT_ID:** It is the unique identifier for an individual patient.\n","*  **HADM_ID:** Represents a single patient‚Äôs admission to the hospital.\n","*   **CHARTDATE:** Gathers the the date on which the note was recorded or created.\n","*   **CHARTTIME:** The specific time (if available) when the note was recorded or created. This column often contains NaN for notes where the exact time isn't recorded.\n","*   **STORETIME:** The time when the note was stored in the database (if available). Often NaN in this dataset.\n","*   **CATEGORY:** The category of the clinical note (e.g., \"Discharge summary,\" \"Nursing,\" etc.), indicating the type or source of the note.\n","*   **DESCRIPTION:** Additional descriptive information about the note (e.g., \"Report\"). Typically, it provides a summary of the note's content or purpose.\n","*   **CGID:** Identifier for the caregiver who wrote the note, if available. Often NaN if the caregiver's ID isn't recorded.\n","*   **ISERROR:** Indicator of whether the note contains an error. Often NaN if errors aren't explicitly flagged.\n","*   **TEXT:** The full text content of the clinical note, including details about the patient's condition, treatment, and other relevant observations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgepgQTWYfjF"},"outputs":[],"source":["# Display the first 5 rows\n","print(noteevents.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NLNz-kmjYtoW"},"outputs":[],"source":["# Display column data types\n","print(noteevents.dtypes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afL_FOOGhyN3"},"outputs":[],"source":["#Check for Missing Chart Time\n","print(f\"Missing CHARTTIME in noteevents: {noteevents['CHARTTIME'].isna().sum()} out of {len(noteevents)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srRS9cihaJzA"},"outputs":[],"source":["# Plot the distribution of note categories\n","noteevents['CATEGORY'].value_counts().plot(kind='bar')\n","plt.title('Distribution of Note Categories')\n","plt.xlabel('Category')\n","plt.ylabel('Count')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qAz4ykszRShM"},"outputs":[],"source":["# View unique values in a column\n","print(noteevents['CATEGORY'].unique())  # List of unique categories\n","\n","# View value counts for a column\n","print(noteevents['TEXT'].value_counts())\n"]},{"cell_type":"code","source":["# Filter noteevents DataFrame (Keep the only the notes that apply to Glucose/Insulin Concepts)\n","categories_to_keep = ['Discharge summary', 'Nursing', 'Physician', 'Nutrition', 'Nursing/other']\n","noteevents = noteevents[noteevents['CATEGORY'].isin(categories_to_keep)]"],"metadata":{"id":"gcKx6huh_yP7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["noteevents.shape"],"metadata":{"id":"kmzSoqruGaze"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOGb4hckRyXl"},"outputs":[],"source":["# Display a sample of note text\n","print(noteevents['TEXT'].iloc[15])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-HFGR_ySfGo"},"outputs":[],"source":["# Display a sample of note text\n","print(noteevents['TEXT'].iloc[1000])  # 1000th note"]},{"cell_type":"markdown","metadata":{"id":"5HD5Z-HqnhjW"},"source":["# UMLS Concept Mapping with ScispaCy Linking"]},{"cell_type":"markdown","source":["## Load SpaCy and ScispaCY with UMLS Linking"],"metadata":{"id":"YXrmu8jP44LD"}},{"cell_type":"code","source":["!pip install scispacy"],"metadata":{"id":"qZlw-nbe44LE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz"],"metadata":{"id":"sNN2jAL844LE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","import scispacy\n","from scispacy.linking import EntityLinker\n","from scispacy.abbreviation import AbbreviationDetector\n","\n","\n","nlp = spacy.load(\"en_core_sci_lg\")\n","print(\"‚úÖ Successfully loaded en_core_sci_lg model.\")\n","\n","# Add abbreviation detector BEFORE linker\n","if \"abbreviation_detector\" not in nlp.pipe_names:\n","    nlp.add_pipe(\"abbreviation_detector\", before=\"ner\")\n","    print(\"‚úÖ Added abbreviation_detector\")\n","\n","\n","# Add UMLS linker if not already in pipeline\n","if \"scispacy_linker\" not in nlp.pipe_names:\n","    nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n","    print(\"‚úÖ Added scispacy_linker\")\n","\n","# Confirm pipeline components\n","print(\"Pipeline:\", nlp.pipe_names)\n","\n","# Confirm linker object\n","linker = nlp.get_pipe(\"scispacy_linker\")\n","print(\"‚úÖ Linker object ready\")"],"metadata":{"id":"oyVyg2Ls44LF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# üîç Test entity linking\n","text = \"Patient received 10 units of insulin for blood glucose of 180.\"\n","doc = nlp(text)\n","\n","print(\"\\nEntities and Linked CUIs:\")\n","for ent in doc.ents:\n","    print(ent.text, ent.label_, ent._.kb_ents)\n","\n","# üîç Manually search UMLS KB for matching CUI entries\n","print(\"\\nMatching CUIs for 'insulin':\")\n","for cui, entity in linker.kb.cui_to_entity.items():\n","    if \"insulin\" in entity.canonical_name.lower():\n","        print(cui, \"‚Üí\", entity.canonical_name)"],"metadata":{"id":"ijAw_Vkz44LF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc = nlp(\"Patient received 10 units of insulin for blood glucose of 180.\")\n","for ent in doc.ents:\n","    print(ent.text, ent.label_, ent._.kb_ents)\n"],"metadata":{"id":"B1FBhSAd44LF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## UMLS Entity Linking Wrapper"],"metadata":{"id":"XXGG2i2O3Q-D"}},{"cell_type":"code","source":["import re\n","import os\n","import pickle\n","import time\n","import pandas as pd\n","from collections import defaultdict\n","from typing import List, Tuple\n","\n","# Define UMLS wrapper class\n","class UMLSLinker:\n","    def __init__(self, model):\n","        self.model = model\n","        self.linker = model.get_pipe(\"scispacy_linker\")\n","\n","    def link_and_save_chunks(self, texts: List[str], ids: List[int], chunk_size: int = 50000,\n","                             batch_size: int = 64, output_dir: str = \"linked_chunks\", verbose: bool = True\n","    ) -> List[Tuple[int, List[Tuple[int, int, str, float]]]]:\n","        os.makedirs(output_dir, exist_ok=True)\n","        all_results = []\n","        total = len(texts)\n","\n","        for start in range(0, total, chunk_size):\n","            end = min(start + chunk_size, total)\n","            chunk_file = os.path.join(output_dir, f\"linked_{start}_{end}.pkl\")\n","\n","            if os.path.exists(chunk_file):\n","                if verbose:\n","                    print(f\"Skipping chunk {start}-{end} (already processed)\")\n","                with open(chunk_file, \"rb\") as f:\n","                    chunk_results = pickle.load(f)\n","            else:\n","                if verbose:\n","                    print(f\"Processing chunk {start}-{end}...\")\n","                chunk = texts[start:end]\n","                chunk_ids = ids[start:end]\n","                chunk_results = self._link_entities(chunk, chunk_ids, batch_size=batch_size, verbose=verbose)\n","                with open(chunk_file, \"wb\") as f:\n","                    pickle.dump(chunk_results, f)\n","\n","            all_results.extend(chunk_results)\n","\n","        return all_results\n","\n","    def _link_entities(self, texts: List[str], ids: List[int], batch_size: int = 32,\n","                       verbose: bool = False) -> List[Tuple[int, List[Tuple[int, int, str, float]]]]:\n","        results = []\n","        start_time = time.time()\n","\n","        for i, doc in enumerate(self.model.pipe(texts, batch_size=batch_size)):\n","            linked = []\n","            for ent in doc.ents:\n","                if ent._.umls_ents:\n","                    cui, score = ent._.umls_ents[0]  # top CUI\n","                    linked.append((ent.start_char, ent.end_char, cui, score))\n","            results.append((ids[i], linked))\n","\n","            if verbose and i % 100 == 0:\n","                elapsed = time.time() - start_time\n","                print(f\"Processed {i} notes in {elapsed:.2f}s\")\n","\n","        return results\n","\n"],"metadata":{"id":"Y2v4ErQt7VnZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize linker object (used for both testing and full dataset):\n","umls_linker = UMLSLinker(nlp)"],"metadata":{"id":"Un_ToueBF4AF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create a CUI Map for Linking"],"metadata":{"id":"Tb_bBwMwNeLr"}},{"cell_type":"code","source":["import re\n","import pandas as pd\n","from collections import defaultdict\n","\n","def build_deduped_cui_map(linker, keywords):\n","    \"\"\"\n","    Builds a deduplicated CUI map dictionary from a list of keywords using UMLS linker.\n","\n","    Args:\n","        linker: ScispaCy UMLS linker object (e.g., umls_linker.linker)\n","        keywords: list of keyword strings\n","\n","    Returns:\n","        dict: A dictionary where keys are CUIs and values are dictionaries\n","              containing 'canonical_name', 'keywords' (as a set), and 'aliases' (as a set).\n","    \"\"\"\n","    matches = {}\n","\n","    for cui, entity in linker.kb.cui_to_entity.items():\n","        canonical = entity.canonical_name.lower()\n","        aliases = [a.lower() for a in getattr(entity, \"aliases\", [])]\n","        all_names = [canonical] + aliases\n","\n","        for keyword in keywords:\n","            pattern = re.compile(rf\"\\b{re.escape(keyword.lower())}\\b\")\n","            if any(pattern.search(name) for name in all_names):\n","                matches[cui.upper()] = {\n","                    \"canonical_name\": entity.canonical_name,\n","                    \"aliases\": entity.aliases,\n","                    \"keyword\": keyword\n","                }\n","                break\n","\n","    # Deduplicate by CUI and aggregate keywords and aliases\n","    deduped = defaultdict(lambda: {\"canonical_name\": None, \"keywords\": set(), \"aliases\": set()})\n","\n","    for cui, entry in matches.items():\n","        deduped[cui][\"canonical_name\"] = entry.get(\"canonical_name\")\n","        deduped[cui][\"keywords\"].add(entry.get(\"keyword\"))\n","        for alias in entry.get(\"aliases\", []):\n","            deduped[cui][\"aliases\"].add(alias)\n","\n","    return dict(deduped) # Return as a regular dictionary"],"metadata":{"id":"i48gVkCfZyue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["glucose_keywords = [\n","    \"blood sugar\", \"FBG\", \"BG\", \"blood glucose\", \"DKA\", \"ketoacidosis\",\n","    \"glucose\", \"hypoglycemia\", \"hypo episode\", \"diabetic coma\",\n","    \"hyperglycemia\", \"glycemia\", \"glycemic\", \"glucose measurement\", \"euglycemia\", \"euglycemic\"\n","]\n"],"metadata":{"id":"ygm7EflsDmYb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["insulin_keywords = [\n","    \"insulin\", \"glargine\", \"lispro\", \"aspart\", \"degludec\", \"isophane\", \"NPH\",\n","    \"basal insulin\", \"bolus insulin\", \"intravenous insulin\", \"continuous insulin infusion\", \"human insulin,\" \"sliding scale\", \"SSI\", \"IV insulin\", \"insulin drip\",\n","    \"humulin\",  \"novolin\", \"lantus\", \"levemir\", \"novolog\", \"humalog\", \"basaglar\", \"toujeo\", \"tresiba\", \"fiasp\", \"afrezza\"\n","]\n"],"metadata":{"id":"iGAExVtUcICd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["glucose_cui_map = build_deduped_cui_map(umls_linker.linker, glucose_keywords)\n","insulin_cui_map = build_deduped_cui_map(umls_linker.linker, insulin_keywords)"],"metadata":{"id":"zYEAlVS4tTW8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cui_dict_to_dataframe(cui_dict):\n","    rows = []\n","    for cui, data in cui_dict.items():\n","        rows.append({\n","            \"CUI\": cui,\n","            \"Canonical Name\": data.get(\"canonical_name\"),\n","            \"Keywords\": \", \".join(sorted(data.get(\"keywords\", []))),\n","            \"Aliases\": \", \".join(sorted(data.get(\"aliases\", [])))\n","        })\n","    return pd.DataFrame(rows)\n"],"metadata":{"id":"6DYywOCcMrfP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["glucose_validation_df = cui_dict_to_dataframe(glucose_cui_map)\n","insulin_validation_df = cui_dict_to_dataframe(insulin_cui_map)\n","\n","\n","\n","save_csv(glucose_validation_df, \"glucose_cui_validation\")\n","save_csv(insulin_validation_df, \"insulin_cui_validation\")\n"],"metadata":{"id":"54qk5CK1pDn-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Keeping a broad CUI list**.\n","\n","\n","Keeping a moderately inclusive keyword list, running tagging, then filtering for context:\n","\n","*   Notes with both glucose mentions and insulin dose adjustments\n","*   Verbs like increase, reduce, adjust, etc.\n"],"metadata":{"id":"_QYEOvSU4czE"}},{"cell_type":"code","source":["#Display and review curated CUI maps\n","print(\"Top Glucose CUIs\")\n","display(glucose_validation_df.sort_values(by=[\"Canonical Name\"]).head(10))\n","\n","print(\"Most Keyword-Rich Glucose CUIs\")\n","glucose_validation_df[\"Keyword Count\"] = glucose_validation_df[\"Keywords\"].apply(lambda x: len(x.split(\", \")))\n","print(glucose_validation_df.sort_values(by=[\"Keyword Count\"], ascending=False).head(10))\n","\n","print(\"Top Insulin CUIs\")\n","display(insulin_validation_df.sort_values(by=[\"Canonical Name\"]).head(10))\n","\n","print(\"Most Keyword-Rich Insulin CUIs\")\n","insulin_validation_df[\"Keyword Count\"] = insulin_validation_df[\"Keywords\"].apply(lambda x: len(x.split(\", \")))\n","print(insulin_validation_df.sort_values(by=[\"Keyword Count\"], ascending=False).head(10))"],"metadata":{"id":"MDaaxMDgGNsB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Reset Chunking to Rerun a Chunk\n","\n","Uncomment to use"],"metadata":{"id":"XRPNInRI913I"}},{"cell_type":"code","source":["def delete_linked_chunk(start: int, end: int, output_dir: str = \"linked_chunks\"):\n","    \"\"\"\n","    Deletes a processed chunk file so it can be reprocessed fresh.\n","    \"\"\"\n","    chunk_file = os.path.join(output_dir, f\"linked_{start}_{end}.pkl\")\n","    if os.path.exists(chunk_file):\n","        os.remove(chunk_file)\n","        print(f\"Deleted: {chunk_file}\")\n","    else:\n","        print(f\"No chunk found at: {chunk_file}\")\n","delete_linked_chunk(0, 1000)\n"],"metadata":{"id":"Uj4sVzF81vzy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Map linked CUIs back to dataframe\n","sample_df = sample_subset.copy()\n","sample_df[\"LINKED_CUIS\"] = sample_df[\"ROW_ID\"].map(dict(linked_sample_results))"],"metadata":{"id":"rDWh9AQSBdty"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Linking CUIs with noteevents (Sample Dataframe 1000 Notes)"],"metadata":{"id":"LxgWDVR1UNiJ"}},{"cell_type":"code","source":["# Sample notes from noteevents\n","sample_subset = noteevents.dropna(subset=[\"TEXT\"]).sample(n=1000, random_state=42)\n","sample_texts = sample_subset[\"TEXT\"].tolist()\n","sample_ids = sample_subset[\"ROW_ID\"].tolist()\n","\n","\n","# Run UMLS linking\n","linked_sample_results = umls_linker.link_and_save_chunks(\n","    texts=sample_texts,\n","    ids=sample_ids,\n","    chunk_size=1000,\n","    batch_size=32\n",")"],"metadata":{"id":"Y3uRBrYG9FW-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clean and Tokenize noteevents Text"],"metadata":{"id":"gbvN4GXhaIyH"}},{"cell_type":"code","source":["# STEP 3b: Clean and tokenize text for statistics\n","import nltk\n","nltk.download('punkt_tab')\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'\\s+', ' ', text)\n","    text = re.sub(r'\\n', ' ', text)\n","    text = re.sub(r'[^\\w\\s.,!?%$]', '', text)\n","    return text.strip()\n","\n","def get_text_stats(text):\n","    tokens = word_tokenize(text)\n","    sents = sent_tokenize(text)\n","    return len(tokens), len(sents), len(set(tokens))\n","\n","sample_df[\"clean_text\"] = sample_df[\"TEXT\"].apply(clean_text)\n","sample_df[[\"token_count\", \"sentence_count\", \"vocab_size\"]] = sample_df[\"clean_text\"].apply(\n","    lambda x: pd.Series(get_text_stats(x))\n",")\n"],"metadata":{"id":"IBj8niBJXwQK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(sample_df.head())"],"metadata":{"id":"Ip9vG-BP4I56"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ‚úÖ Count how many linked CUIs per note\n","sample_df[\"linked_cui_count\"] = sample_df[\"LINKED_CUIS\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n","\n","# ‚úÖ Summarize distribution\n","linked_cui_summary = sample_df[\"linked_cui_count\"].value_counts().sort_index()\n","summary_df = pd.DataFrame({\n","    \"CUIs Linked\": linked_cui_summary.index,\n","    \"Note Count\": linked_cui_summary.values\n","})\n","display(summary_df)\n","\n","# ‚úÖ Preview some non-empty results\n","display(sample_df[sample_df[\"linked_cui_count\"] > 0][[\"ROW_ID\", \"TEXT\", \"LINKED_CUIS\"]].head())\n"],"metadata":{"id":"SJdeRj1ImRGf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(nlp.pipe_names)\n"],"metadata":{"id":"k7gewj9vmjpp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Text Data Visualization\n","plt.figure()\n","sample_df[\"token_count\"].hist(bins=50)\n","plt.title(\"Distribution of Token Count per Note\")\n","plt.xlabel(\"Token Count\")\n","plt.ylabel(\"Frequency\")\n","plt.tight_layout()\n","plt.show()\n","\n","plt.figure()\n","sample_df[\"sentence_count\"].hist(bins=50)\n","plt.title(\"Distribution of Sentence Count per Note\")\n","plt.xlabel(\"Sentence Count\")\n","plt.ylabel(\"Frequency\")\n","plt.tight_layout()\n","plt.show()\n","\n","plt.figure()\n","sample_df[\"vocab_size\"].hist(bins=50)\n","plt.title(\"Distribution of Vocabulary Size per Note\")\n","plt.xlabel(\"Unique Token Count\")\n","plt.ylabel(\"Frequency\")\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"DyPnpU-VYqFE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tagging Notes"],"metadata":{"id":"UfLOkoXGVP0y"}},{"cell_type":"code","source":["\n","\n","\n","def tag_note_with_concepts_with_spans(linked_cuis, glucose_cuis, insulin_cuis):\n","    glucose_cuis = {c.upper() for c in glucose_cuis}\n","    insulin_cuis = {c.upper() for c in insulin_cuis}\n","    tags = []\n","    for start, end, cui, score in linked_cuis:\n","        cui = cui.upper()\n","        if cui in glucose_cuis:\n","            tags.append((start, end, \"glucose\"))\n","        elif cui in insulin_cuis:\n","            tags.append((start, end, \"insulin\"))\n","    return tags\n","\n","def build_spans(row):\n","    return tag_note_with_concepts_with_spans(\n","        row[\"LINKED_CUIS\"],\n","        glucose_cui_map.keys(),\n","        insulin_cui_map.keys()\n","    )"],"metadata":{"id":"wnSNvfd84TCY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply tagging\n","sample_df[\"matched_spans\"] = sample_df.apply(build_spans, axis=1)\n","sample_df[\"tags\"] = sample_df[\"matched_spans\"].apply(lambda spans: list({label for _, _, label in spans}))\n","\n"],"metadata":{"id":"AoxAHFz60CAb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def classify_tag(spans):\n","    tags = {label for _, _, label in spans}\n","    if tags == {\"glucose\"}:\n","        return \"glucose only\"\n","    elif tags == {\"insulin\"}:\n","        return \"insulin only\"\n","    elif \"glucose\" in tags and \"insulin\" in tags:\n","        return \"both\"\n","    return \"none\"\n","\n","sample_df[\"tag_category\"] = sample_df[\"matched_spans\"].apply(classify_tag)"],"metadata":{"id":"TY6a3_-y3ZaE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# STEP 5: Coverage Analysis\n","\n","def summarize_cui_counts(df, linker, column=\"LINKED_CUIS\"):\n","    filtered_cuis = [cui for row in df[column] for cui in row if cui[3] >= min_score]\n","    cuis = [cui[2] for cui in filtered_cuis]\n","    counts = pd.Series(cuis).value_counts().reset_index()\n","    counts.columns = [\"cui\", \"count\"]\n","    counts[\"name\"] = counts[\"cui\"].apply(\n","        lambda cui: linker.linker.kb.cui_to_entity[cui].canonical_name\n","        if cui in linker.linker.kb.cui_to_entity else \"UNKNOWN\"\n","    )\n","    return counts"],"metadata":{"id":"75Nm9TjzHL8y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["glucose_summary = summarize_cui_counts(sample_df, umls_linker)\n","glucose_summary = glucose_summary[glucose_summary[\"cui\"].isin(glucose_cui_map.keys())]\n","\n","insulin_summary = summarize_cui_counts(sample_df, umls_linker)\n","insulin_summary = insulin_summary[insulin_summary[\"cui\"].isin(insulin_cui_map.keys())]"],"metadata":{"id":"V7vWcZcmbMn8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# STEP 6: Visualize Top CUIs by Count (Score ‚â• 0.85 vs 0.90)\n","\n","import matplotlib.pyplot as plt\n","\n","# Plot glucose CUIs at score ‚â• 0.85\n","plt.figure()\n","glucose_summary.head(10).plot(kind=\"bar\", x=\"name\", y=\"count\", legend=False)\n","plt.title(\"Top Glucose CUIs (score ‚â• 0.85)\")\n","plt.xlabel(\"UMLS Concept\")\n","plt.ylabel(\"Count\")\n","plt.xticks(rotation=45, ha=\"right\")\n","plt.tight_layout()\n","plt.show()\n","\n","# Plot glucose CUIs at score ‚â• 0.90\n","glucose_summary_90 = summarize_cui_counts(sample_df, umls_linker, min_score=0.90)\n","glucose_summary_90 = glucose_summary_90[glucose_summary_90[\"cui\"].isin(glucose_cui_map.keys())]\n","\n","plt.figure()\n","glucose_summary_90.head(10).plot(kind=\"bar\", x=\"name\", y=\"count\", legend=False, color=\"teal\")\n","plt.title(\"Top Glucose CUIs (score ‚â• 0.90)\")\n","plt.xlabel(\"UMLS Concept\")\n","plt.ylabel(\"Count\")\n","plt.xticks(rotation=45, ha=\"right\")\n","plt.tight_layout()\n","plt.show()\n","\n","# Plot insulin CUIs at score ‚â• 0.85\n","plt.figure()\n","insulin_summary.head(10).plot(kind=\"bar\", x=\"name\", y=\"count\", legend=False, color=\"orange\")\n","plt.title(\"Top Insulin CUIs (score ‚â• 0.85)\")\n","plt.xlabel(\"UMLS Concept\")\n","plt.ylabel(\"Count\")\n","plt.xticks(rotation=45, ha=\"right\")\n","plt.tight_layout()\n","plt.show()\n","\n","# Plot insulin CUIs at score ‚â• 0.90\n","insulin_summary_90 = summarize_cui_counts(sample_df, umls_linker, min_score=0.90)\n","insulin_summary_90 = insulin_summary_90[insulin_summary_90[\"cui\"].isin(insulin_cui_map.keys())]\n","\n","plt.figure()\n","insulin_summary_90.head(10).plot(kind=\"bar\", x=\"name\", y=\"count\", legend=False, color=\"purple\")\n","plt.title(\"Top Insulin CUIs (score ‚â• 0.90)\")\n","plt.xlabel(\"UMLS Concept\")\n","plt.ylabel(\"Count\")\n","plt.xticks(rotation=45, ha=\"right\")\n","plt.tight_layout()\n","plt.show()\n","\n","# Result: sample_df now has LINKED_CUIS, matched_spans, tags, tag_category\n"],"metadata":{"id":"ohtJecFzHRfk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary = summarize_cui_counts(sample_df, linker=umls_linker)\n","display(summary)"],"metadata":{"id":"8wtOa2uQHium"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Total unique validated glucose CUIs that appeared\n","print(\"Glucose CUIs matched:\", glucose_summary['cui'].nunique())\n","\n","# Total mentions (sum of counts across CUIs)\n","print(\"Total glucose mentions:\", glucose_summary['count'].sum())\n","\n","# Same for insulin\n","print(\"Insulin CUIs matched:\", insulin_summary['cui'].nunique())\n","print(\"Total insulin mentions:\", insulin_summary['count'].sum())"],"metadata":{"id":"8sFXVYanH0az"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n","tagged_notes = sample_df[sample_df[\"tag_category\"] != \"none\"].copy()\n"],"metadata":{"id":"6zN1BLkkAt_9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Display 3 Sample Clinical Notes with Tags"],"metadata":{"id":"Lfjm_Y8cGHWH"}},{"cell_type":"code","source":["from spacy import displacy\n","from IPython.display import display, HTML\n","\n","def visualize_tagged_note(text, spans):\n","    ents = [{\"start\": s, \"end\": e, \"label\": l} for s, e, l in spans]\n","    html = displacy.render({\"text\": text, \"ents\": ents, \"title\": None}, style=\"ent\", manual=True)\n","    display(HTML(html))\n","\n","# Show 3 examples from each tag class\n","for category in [\"glucose only\", \"insulin only\", \"both\"]:\n","    subset = tagged_notes[tagged_notes[\"tag_category\"] == category].head(3)\n","    for _, row in subset.iterrows():\n","        print(f\"\\nüìù ROW_ID: {row['ROW_ID']} ({category})\")\n","        visualize_tagged_note(row[\"TEXT\"], row[\"matched_spans\"])\n"],"metadata":{"id":"IjlrkrTzA3Y3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","tagged_notes[\"tag_category\"].value_counts().plot(kind=\"bar\", color=[\"red\", \"blue\", \"purple\"])\n","plt.title(\"Note Classification by UMLS Tags\")\n","plt.xlabel(\"Tag Category\")\n","plt.ylabel(\"Number of Notes\")\n","plt.xticks(rotation=0)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"-YzWkRyGBom5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","def plot_top_umls(summary_df, top_n=20):\n","    top = summary_df.head(top_n)\n","    plt.figure(figsize=(10, 6))\n","    plt.barh(top[\"name\"], top[\"count\"])\n","    plt.gca().invert_yaxis()\n","    plt.xlabel(\"Mention Count\")\n","    plt.title(f\"Top {top_n} UMLS Concepts (Score ‚â• 0.85)\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_top_umls(summary)\n"],"metadata":{"id":"WkvrnLeTHqg0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Visualize Note Classifications"],"metadata":{"id":"Tfg9qj4mGdyG"}},{"cell_type":"code","source":["# Compute tag list from matched spans\n","sample_df[\"tags\"] = sample_df[\"matched_spans\"].apply(lambda spans: list({label for _, _, label in spans}))\n","\n","# Filter to only tagged notes\n","tagged_notes = sample_df[sample_df[\"tag_category\"] != \"none\"].copy()\n","\n","# Clean and compute text stats\n","tagged_notes[\"raw_text\"] = tagged_notes[\"TEXT\"]\n","tagged_notes[\"clean_text\"] = tagged_notes[\"TEXT\"].apply(clean_text)\n","tagged_notes[[\"token_count\", \"sentence_count\", \"vocab_size\"]] = tagged_notes[\"clean_text\"].apply(\n","    lambda x: pd.Series(get_text_stats(x))\n",")\n","\n","# Canonical name from top CUI\n","tagged_notes[\"canonical_name\"] = tagged_notes[\"top_cui\"].apply(\n","    lambda cui: get_canonical_name(cui, umls_linker.linker)\n",")\n","\n","\n","\n","\n","# ‚úÖ Final DataFrame for merge\n","final_notes_df = tagged_notes[[\n","    \"ROW_ID\", \"raw_text\", \"clean_text\", \"LINKED_CUIS\",\n","    \"matched_spans\", \"tag_category\", \"tags\",\n","    \"top_cui\", \"canonical_name\",  # ‚úÖ keep only useful UMLS output\n","    \"token_count\", \"sentence_count\", \"vocab_size\"\n","]]\n","\n","save_csv(final_notes_df, \"final_tagged_notes_preprocessed\")\n","\n"],"metadata":{"id":"U_WrdPN5QFzs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tagged_notes.head()"],"metadata":{"id":"Y16jHW1-gCjP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#note_type from noteevents using ROW_ID\n","tagged_notes = tagged_notes.merge(\n","    noteevents[[\"ROW_ID\", \"CATEGORY\"]],\n","    how=\"left\", on=\"ROW_ID\"\n",").rename(columns={\"CATEGORY\": \"note_type\"})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":207},"id":"-4oQk4J0lk3H","executionInfo":{"status":"error","timestamp":1753674529986,"user_tz":240,"elapsed":118,"user":{"displayName":"April Taylor","userId":"05794515634141358651"}},"outputId":"45f55575-7752-4132-dff5-a026bce8e9dd"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'tagged_notes' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-77-537077769.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#note_type from noteevents using ROW_ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m tagged_notes = tagged_notes.merge(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnoteevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ROW_ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CATEGORY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ROW_ID\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ).rename(columns={\"CATEGORY\": \"note_type\"})\n","\u001b[0;31mNameError\u001b[0m: name 'tagged_notes' is not defined"]}]},{"cell_type":"code","source":["tagged_notes[\"note_type\"].value_counts().plot(kind=\"bar\", color=\"teal\")\n","plt.title(\"Tagged Notes by Note Type\")\n","plt.xlabel(\"Note Type\")\n","plt.ylabel(\"Number of Notes\")\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"-5C6hNkyUws2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# From top glucose mentions:\n","priority_glucose_cuis = {\n","    'C0017741',  # Glucose tolerance test\n","    'C0017725',  # glucose\n","    'C0011880',  # Diabetic Ketoacidosis\n","    'C0687720',  # Central Diabetes Insipidus\n","    'C0268186',  # Congenital glucose-galactose malabsorption\n","    # ... add others after manual review\n","}\n"],"metadata":{"id":"PBsm-0_pVPo6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["matched = linked_cuis_in_notes.intersection(priority_glucose_cuis)\n","coverage = len(matched) / len(priority_glucose_cuis) * 100\n","print(f\"‚úÖ Realistic Glucose Coverage: {coverage:.1f}% ({len(matched)} of {len(priority_glucose_cuis)} expected CUIs)\")\n"],"metadata":{"id":"nmDZxRIGtns6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","# Step 1: Flatten all matched CUIs from tagged notes\n","matched_cuis = [\n","    cui\n","    for row in tagged_notes.itertuples()\n","    for start, end, label in row.matched_spans\n","    for cui, *_ in row.LINKED_CUIS\n","    if label in row.tags and cui in curated_cuis\n","]\n","\n","# Step 2: Count frequencies\n","matched_cui_counts = Counter(matched_cuis)\n","\n","# Step 3: Build table with canonical names\n","top_matched = matched_cui_counts.most_common(20)\n","top_df = pd.DataFrame(top_matched, columns=[\"CUI\", \"Count\"])\n","top_df[\"Name\"] = top_df[\"CUI\"].apply(lambda cui: get_canonical_name(cui, umls_linker.linker))\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Optional: Visualize\n","plt.figure(figsize=(10, 6))\n","sns.barplot(data=top_df, x=\"Count\", y=\"Name\", palette=\"magma\")\n","plt.title(\"üî¨ Top 20 Matched Curated CUIs in Tagged Notes\")\n","plt.xlabel(\"Frequency\")\n","plt.ylabel(\"UMLS Concept\")\n","plt.tight_layout()\n","plt.show()\n","\n","# Show top 20 CUIs in notebook\n","display(top_df)\n","\n","# Save as CSV\n","save_csv(top_df, \"top_curated_cuis_matched\")"],"metadata":{"id":"PBRyWkFBdl9g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Label Clinical Notes (using raw text) - Full Dataset"],"metadata":{"id":"2_9dolVbPtFJ"}},{"cell_type":"code","source":["# Prepare the linker\n","umls_linker = UMLSLinker(nlp)\n","\n","# Prepare the input texts\n","texts = noteevents[\"TEXT\"].dropna().tolist()\n","\n","# Run the UMLS linking with batch size and progress print\n","linked_results = umls_linker.link_and_save_chunks(\n","    texts=noteevents[\"TEXT\"].dropna().tolist(),\n","    chunk_size=50000,\n","    batch_size=64\n",")\n","\n","# Save results back to your dataframe\n","noteevents[\"LINKED_CUIS\"] = pd.Series([cuis for _, cuis in linked_results])\n"],"metadata":{"id":"nLWyGPt66kSq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def merge_notes_with_structured(noteevents: pd.DataFrame, insulin_data: pd.DataFrame) -> pd.DataFrame:\n","    # Convert to datetime and ensure timezone-naive\n","    noteevents['DATETIME'] = pd.to_datetime(\n","        noteevents['CHARTDATE'].astype(str) + ' ' + noteevents['CHARTTIME'].astype(str), errors='coerce')\n","    noteevents['DATETIME'] = noteevents['DATETIME'].dt.tz_localize(None)\n","    noteevents['CHARTDATE'] = noteevents['DATETIME'].dt.date\n","\n","    insulin_data['TIMER'] = pd.to_datetime(insulin_data['TIMER'], errors='coerce').dt.tz_localize(None)\n","    insulin_data['CHARTDATE'] = insulin_data['TIMER'].dt.date\n","\n","    # Separate valid and invalid HADM_IDs\n","    valid_hadm_notes = noteevents[noteevents['HADM_ID'] != -1].copy().sort_values('CHARTDATE')\n","    invalid_hadm_notes = noteevents[noteevents['HADM_ID'] == -1].copy().sort_values('CHARTDATE')\n","    insulin_data = insulin_data.sort_values('CHARTDATE')\n","\n","    # Merge valid on SUBJECT_ID, HADM_ID, CHARTDATE\n","    merged_valid = pd.merge(\n","        valid_hadm_notes,\n","        insulin_data,\n","        on=['SUBJECT_ID', 'HADM_ID', 'CHARTDATE'],\n","        how='inner'\n","    )\n","\n","    # Merge invalid on SUBJECT_ID, CHARTDATE only\n","    merged_invalid = pd.merge(\n","        invalid_hadm_notes,\n","        insulin_data,\n","        on=['SUBJECT_ID', 'CHARTDATE'],\n","        how='inner'\n","    )\n","\n","    return pd.concat([merged_valid, merged_invalid], ignore_index=True)"],"metadata":{"id":"VqID-nhH6qKS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vveAwbnT11T4"},"source":["# NER Modelling"]},{"cell_type":"code","source":["# Prepare data for NER\n","texts = merged_data[\"TEXT\"].apply(str.split).tolist()\n","labels = merged_data[\"UMLS_Annotations_Numeric\"].tolist()"],"metadata":{"id":"olf_qHtbUwmk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n","import torch\n","\n","# Initialize ClinicalBERT tokenizer and device\n","tokenizer = BertTokenizerFast.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"sDTsIUoBYazA","colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["de000d9414d848ecbde0f1a31fb74237","786ae788b034427aa2aac1a457a7a91c","0786eb297c674385a44121ae1b658ca9","9fd573a3c606437388ab208b9ba9a462","9fc07b87a76d4914a9612bb77cdd08fe","2c8aaa15514a445191822b448c8240a9","40effbe5d61542f3b258b33f69b351b3","6ad2852dae0541c1984aa9e959051d49","acc9edb05e01430094c3a4f1ce56f8b3","848d475dab094f51afbf429d1e2802df","8b05fdd593b749b78f3fba82fa939078","f66f6b6d46ec472db840f9e6c46dd815","ab9e4059d2224a5c98869ba686a44ea7","a6de46a8a18b4344892cb5d53f9601e6","0e720df05be347eabb6331434ab29009","cc05e3ec0b4d4e28a98ff2b4ba4dc917","3fd1caba5325448ea1b4a273890c1410","741f91e293364837ac8300b270cadc03","9580dacc12e24f43b0d719d0325a7561","57061dadcd494985a3f80d33de66aa79","2c6cded29d6141d3a7328e10d50cdb2a","0e852b49cb62423d9c04bffa3eda988f"]},"executionInfo":{"status":"ok","timestamp":1753058536752,"user_tz":240,"elapsed":20052,"user":{"displayName":"April Taylor","userId":"05794515634141358651"}},"outputId":"fc114173-7395-4f37-e36d-0d96a1827455"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["vocab.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de000d9414d848ecbde0f1a31fb74237"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f66f6b6d46ec472db840f9e6c46dd815"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"QuRm-oB6QmjG"},"source":["## Data Tokenization and Labeling"]},{"cell_type":"markdown","metadata":{"id":"AiT4_VqNWcj-"},"source":["### Tokenize"]},{"cell_type":"code","source":["# Function to align labels with tokenized inputs\n","def align_labels_with_tokens(labels, word_ids):\n","    aligned_labels = []\n","    current_word_idx = 0\n","    for word_id in word_ids:\n","        if word_id is None:\n","            aligned_labels.append(-100)  # Ignore special tokens\n","        elif word_id == current_word_idx:\n","            aligned_labels.append(labels[word_id] if word_id < len(labels) else -100)\n","            current_word_idx += 1\n","        else:\n","            aligned_labels.append(labels[current_word_idx - 1] if current_word_idx > 0 else -100)\n","    return aligned_labels\n"],"metadata":{"id":"6hdiAdXABiXY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to tokenize text and align labels\n","def tokenize_and_align_labels(texts, labels):\n","    tokenized_inputs = tokenizer(\n","        texts,\n","        padding=True,\n","        truncation=True,\n","        max_length=128,  # Adjust as needed\n","        return_tensors=\"pt\",\n","        is_split_into_words=True,\n","    )\n","    aligned_labels_list = []\n","    for i, label in enumerate(labels):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        aligned_labels = align_labels_with_tokens(label, word_ids)\n","        aligned_labels_list.append(aligned_labels)\n","    return tokenized_inputs, aligned_labels_list\n","\n","    tokenized_inputs, aligned_labels = tokenize_and_align_labels(texts, labels)"],"metadata":{"id":"AOngJCgXYj5k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset, Subset # Import Dataset and Subset\n","from sklearn.model_selection import train_test_split # Import train_test_split\n","\n","class ClinicalNERDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","    def __getitem__(self, idx):\n","        return {\n","            \"input_ids\": self.encodings[\"input_ids\"][idx],\n","            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n","            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n","        }\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Call the function to get tokenized inputs and aligned labels\n","tokenized_inputs, aligned_labels = tokenize_and_align_labels(texts, labels)\n","\n","dataset = ClinicalNERDataset(tokenized_inputs, aligned_labels)\n","train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\n","train_dataset = Subset(dataset, train_idx)\n","test_dataset = Subset(dataset, test_idx)"],"metadata":{"id":"nNweAKpQVByv","colab":{"base_uri":"https://localhost:8080/","height":207},"executionInfo":{"status":"error","timestamp":1753058583662,"user_tz":240,"elapsed":269,"user":{"displayName":"April Taylor","userId":"05794515634141358651"}},"outputId":"97068648-6488-4888-dfd3-6992144694a9"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'tokenized_inputs' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-5-1886913547.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClinicalNERDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maligned_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenized_inputs' is not defined"]}]},{"cell_type":"code","source":["# Training function\n","def train_ner_model(train_dataset, model, learning_rate=5e-5, epochs=3):\n","    training_args = TrainingArguments(\n","        output_dir=\"./clinicalbert_results\",\n","        num_train_epochs=epochs,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=16,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        logging_dir=\"./clinicalbert_logs\",\n","        logging_steps=10,\n","        learning_rate=learning_rate,\n","        load_best_model_at_end=True,\n","    )\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        tokenizer=tokenizer,\n","    )\n","    trainer.train()\n","    return trainer"],"metadata":{"id":"D9-xPFAjY1p_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate ClinicalBERT\n","predictions, labels, _ = trainer.predict(test_dataset)\n","predictions = predictions.argmax(axis=-1)\n","\n","true_labels = []\n","pred_labels = []\n","for pred, label in zip(predictions, labels):\n","    for p, l in zip(pred, label):\n","        if l != -100:\n","            true_labels.append(id_to_label[l])\n","            pred_labels.append(id_to_label[p])\n","\n","report = classification_report(true_labels, pred_labels, zero_division=0)\n","accuracy = accuracy_score(true_labels, pred_labels)\n","precision = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n","recall = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n","f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n","\n","print(\"NER Model Evaluation:\")\n","print(report)\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1 Score: {f1:.4f}\")\n"],"metadata":{"id":"T2CIWedsVFKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","import torch.nn as nn\n","\n","# Check if GPU is available, otherwise use CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\") # Print the device being used\n","\n","\n","# Number of epochs\n","num_epochs = 3  # Adjust as needed\n","\n","# Move model to the correct device\n","model.to(device)\n","\n","# Loss tracker\n","training_loss = []\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","\n","    # Use dataloader instead of data_loader in the loop\n","    for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"): # Change data_loader to dataloader\n","        # Move batch to device\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        # Zero gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","\n","        # Compute loss\n","        loss = outputs.loss\n","        epoch_loss += loss.item()\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","    # Track and log loss\n","    avg_loss = epoch_loss / len(dataloader) # Change data_loader to dataloader\n","    training_loss.append(avg_loss)\n","    print(f\"Epoch {epoch + 1} Loss: {avg_loss:.4f}\")"],"metadata":{"id":"XjgSez_3NWIQ","colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"status":"error","timestamp":1753058604439,"user_tz":240,"elapsed":63,"user":{"displayName":"April Taylor","userId":"05794515634141358651"}},"outputId":"ddfa4e82-f967-4410-fa61-e8eb00586164"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-7-3147325568.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Move model to the correct device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Loss tracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"YevaHNOWRg07"},"source":["## Split data and Create dataset for Training"]},{"cell_type":"markdown","metadata":{"id":"CGo9ChgURqN9"},"source":["## Train ClinicalBert for NER"]},{"cell_type":"markdown","metadata":{"id":"Gp6ciN2LpImU"},"source":["## Evaluate the NER Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ow6Lf7d2KoMC"},"outputs":[],"source":["from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n","\n","# Evaluation function\n","def evaluate_ner_model(trainer, test_dataset, id_to_label):\n","    predictions, labels, _ = trainer.predict(test_dataset)\n","    predictions = predictions.argmax(axis=-1)\n","\n","    true_labels = []\n","    pred_labels = []\n","\n","    for pred, label in zip(predictions, labels):\n","        for p, l in zip(pred, label):\n","            if l != -100:\n","                true_labels.append(id_to_label[l])\n","                pred_labels.append(id_to_label[p])\n","\n","    report = classification_report(true_labels, pred_labels, zero_division=0)\n","    accuracy = accuracy_score(true_labels, pred_labels)\n","    precision = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n","    recall = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n","    f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n","\n","    print(\"NER Model Evaluation:\")\n","    print(report)\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","\n","# Evaluate Model\n","evaluate_ner_model(trainer, test_dataset, id_to_label)\n"]},{"cell_type":"markdown","metadata":{"id":"JCV_Us8c1UVe"},"source":["\n","# Relation Extraction"]},{"cell_type":"code","source":["def prepare_relation_data_fixed(data, umls_dict):\n","    pairs, labels = [], []\n","    for _, row in data.iterrows():\n","        entities = row['UMLS_Annotations']\n","        if not entities:\n","            continue\n","        for i, entity1 in enumerate(entities):\n","            for j, entity2 in enumerate(entities):\n","                if i < j:  # Avoid redundancy (entity1, entity2) == (entity2, entity1)\n","                    pairs.append((entity1, entity2))\n","                    labels.append(1 if entity1 == entity2 else 0)\n","    return pairs, labels\n","\n","pairs, labels = prepare_relation_data(merged_data, umls_dict)\n","\n","# Create a custom dataset class\n","class RelationDataset(Dataset):\n","    def __init__(self, pairs, labels, tokenizer):\n","        self.pairs = pairs\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, idx):\n","        entity1, entity2 = self.pairs[idx]  # Unpack the tuple\n","        label = self.labels[idx]\n","\n","        # Tokenize the pair using the [SEP] token\n","        encoding = self.tokenizer(\n","            entity1,\n","            entity2,\n","            add_special_tokens=True,\n","            padding='max_length',\n","            truncation=True,\n","            max_length=128,  # Adjust as needed\n","            return_tensors='pt'\n","        )\n","\n","        # Return a dictionary\n","        return {\n","            'input_ids': encoding['input_ids'].squeeze(),\n","            'attention_mask': encoding['attention_mask'].squeeze(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# Initialize tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n","\n","# Create Dataset objects\n","train_dataset = RelationDataset(pairs, labels, tokenizer)\n","# Assuming you want to use the same data for evaluation,\n","# you can create a separate eval_dataset or use train_dataset\n","eval_dataset = RelationDataset(pairs, labels, tokenizer)\n","\n","# Use BertForSequenceClassification for relation extraction\n","from transformers import BertForSequenceClassification # Import BertForSequenceClassification\n","relation_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","\n","relation_training_args = TrainingArguments(\n","    output_dir=\"./relation_results\",\n","    evaluation_strategy=\"epoch\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=64\n",")\n","relation_trainer = Trainer(\n","    model=relation_model,\n","    args=relation_training_args,\n","    train_dataset=train_dataset,  # Pass the Dataset object\n","    eval_dataset=eval_dataset     # Pass the Dataset object\n",")\n","relation_trainer.train"],"metadata":{"id":"sxLBhtth3_lL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["relation_eval = relation_trainer.evaluate()\n","print(\"Relation Extraction Results:\", relation_eval)\n","\n","# Visualize RE Results\n","plt.figure(figsize=(10, 6))\n","sns.histplot([f\"{pair}: {label}\" for pair, label in zip(pairs, labels)], kde=False)\n","plt.title('Common Relationships')\n","plt.xlabel('Relationship')\n","plt.ylabel('Frequency')\n","plt.show()"],"metadata":{"id":"Oi3u0r6N_Ny3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset\n","import torch\n","\n","# Initialize ClinicalBERT tokenizer and device\n","tokenizer = BertTokenizerFast.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Function to align labels with tokenized inputs\n","def align_labels_with_tokens(labels, word_ids):\n","    aligned_labels = []\n","    current_word_idx = 0\n","    for word_id in word_ids:\n","        if word_id is None:\n","            aligned_labels.append(-100)  # Ignore special tokens\n","        elif word_id == current_word_idx:\n","            aligned_labels.append(labels[word_id] if word_id < len(labels) else -100)\n","            current_word_idx += 1\n","        else:\n","            aligned_labels.append(labels[current_word_idx - 1] if current_word_idx > 0 else -100)\n","    return aligned_labels\n","\n","# Function to tokenize text and align labels\n","def tokenize_and_align_labels(texts, labels):\n","    tokenized_inputs = tokenizer(\n","        texts,\n","        padding=True,\n","        truncation=True,\n","        max_length=128,  # Adjust as needed\n","        return_tensors=\"pt\",\n","        is_split_into_words=True,\n","    )\n","    aligned_labels_list = []\n","    for i, label in enumerate(labels):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        aligned_labels = align_labels_with_tokens(label, word_ids)\n","        aligned_labels_list.append(aligned_labels)\n","    return tokenized_inputs, aligned_labels_list\n","\n","# Dataset class\n","class NERDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return item\n","\n","# Example texts and labels\n","texts = [\n","    \"Patient presented with hyperglycemia.\",\n","    \"Administered insulin and rechecked glucose levels.\",\n","    \"The glucose reading was 150 mg/dL.\",\n","    \"No signs of hypoglycemia were noted.\",\n","]\n","labels = [\n","    [0, 3, 0, 0, 4, 0],   # Example labels for the first sentence\n","    [0, 6, 0, 0, 0, 4, 0],  # Example labels for the second sentence\n","    [0, 0, 4, 0, 0, 0, 0, 0],  # Example labels for the third sentence\n","    [0, 0, 0, 3, 0, 0],   # Example labels for the fourth sentence\n","]\n","\n","# Train-Test Split\n","texts_train, texts_test, labels_train, labels_test = train_test_split(\n","    texts, labels, test_size=0.2, random_state=42\n",")\n","\n","# Tokenize and align labels\n","train_encodings, train_labels = tokenize_and_align_labels(texts_train, labels_train)\n","test_encodings, test_labels = tokenize_and_align_labels(texts_test, labels_test)\n","\n","# Prepare datasets\n","train_dataset = NERDataset(train_encodings, train_labels)\n","test_dataset = NERDataset(test_encodings, test_labels)\n","\n","# Load ClinicalBERT model\n","num_labels = 12 # Replace with the actual number of labels\n","model = BertForTokenClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=num_labels)\n","model.to(device)\n","\n","# Training function\n","def train_ner_model(train_dataset, test_dataset, model, learning_rate=5e-5, epochs=3):\n","    training_args = TrainingArguments(\n","        output_dir=\"./clinicalbert_results\",\n","        num_train_epochs=epochs,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=16,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        logging_dir=\"./clinicalbert_logs\",\n","        logging_steps=10,\n","        learning_rate=learning_rate,\n","        load_best_model_at_end=True,\n","    )\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=test_dataset,\n","        tokenizer=tokenizer,\n","    )\n","    trainer.train()\n","    return trainer\n","\n","# Train the model\n","trainer = train_ner_model(train_dataset, test_dataset, model, learning_rate=5e-5, epochs=3)\n"],"metadata":{"id":"tOYf4OBBRiDU"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d24df807","executionInfo":{"status":"ok","timestamp":1753818142423,"user_tz":240,"elapsed":83442,"user":{"displayName":"April Taylor","userId":"05794515634141358651"}},"outputId":"fcae515d-4273-4cbf-d91d-dcffa68b2072"},"source":["# Install a specific version of scikit-learn that might be compatible with the scispacy model assets\n","!pip install scikit-learn==1.1.2"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-learn==1.1.2\n","  Downloading scikit-learn-1.1.2.tar.gz (7.0 MB)\n","\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/7.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.6/7.0 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m \u001b[32m6.9/7.0 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m√ó\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m√ó\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a57b8871","executionInfo":{"status":"ok","timestamp":1753818369008,"user_tz":240,"elapsed":25288,"user":{"displayName":"April Taylor","userId":"05794515634141358651"}},"outputId":"f71a8c50-40a5-4020-8827-499cc76440ad"},"source":["# --- Clean Install scispacy and Model ---\n","# This cell attempts to create a clean environment for scispacy and en_core_sci_lg\n","\n","# Uninstall potentially conflicting packages\n","!pip uninstall -y spacy scispacy\n","\n","# Install the specific en_core_sci_lg model.\n","# This should automatically install compatible versions of scispacy and spacy.\n","!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz\n","\n","# Ensure scikit-learn is at least the required version for scispacy 0.5.1\n","# scispacy 0.5.1 requires scikit-learn>=0.20.3\n","# We don't install a fixed version due to previous build issues, but ensure the minimum requirement is met\n","!pip install \"scikit-learn>=0.20.3\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: spacy 3.4.4\n","Uninstalling spacy-3.4.4:\n","  Successfully uninstalled spacy-3.4.4\n","\u001b[33mWARNING: Skipping scispacy as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCollecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz\n","  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz (532.3 MB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting spacy<3.5.0,>=3.4.1 (from en_core_sci_lg==0.5.1)\n","  Using cached spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (1.0.13)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (3.0.10)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (8.1.12)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (0.10.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (2.0.10)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (0.7.0)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (0.11.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (4.67.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (1.26.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (1.10.22)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (3.1.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (75.2.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (25.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (3.5.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (1.3.0)\n","Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.11/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (0.1.1)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (4.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (2025.7.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (0.1.5)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (8.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_core_sci_lg==0.5.1) (1.2.1)\n","Using cached spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n","Installing collected packages: spacy\n","Successfully installed spacy-3.4.4\n","Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.3) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.3) (1.16.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.3) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.3) (3.6.0)\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"toc_visible":true,"collapsed_sections":["z-2Oq724hBu_","2g_TnazDiqAm","d2I9iC4JMKi9","qA50KWZsgDo2","yVu0iiDtkfZw"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"de000d9414d848ecbde0f1a31fb74237":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_786ae788b034427aa2aac1a457a7a91c","IPY_MODEL_0786eb297c674385a44121ae1b658ca9","IPY_MODEL_9fd573a3c606437388ab208b9ba9a462"],"layout":"IPY_MODEL_9fc07b87a76d4914a9612bb77cdd08fe"}},"786ae788b034427aa2aac1a457a7a91c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c8aaa15514a445191822b448c8240a9","placeholder":"‚Äã","style":"IPY_MODEL_40effbe5d61542f3b258b33f69b351b3","value":"vocab.txt:‚Äá"}},"0786eb297c674385a44121ae1b658ca9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ad2852dae0541c1984aa9e959051d49","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_acc9edb05e01430094c3a4f1ce56f8b3","value":1}},"9fd573a3c606437388ab208b9ba9a462":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_848d475dab094f51afbf429d1e2802df","placeholder":"‚Äã","style":"IPY_MODEL_8b05fdd593b749b78f3fba82fa939078","value":"‚Äá213k/?‚Äá[00:00&lt;00:00,‚Äá12.7MB/s]"}},"9fc07b87a76d4914a9612bb77cdd08fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c8aaa15514a445191822b448c8240a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40effbe5d61542f3b258b33f69b351b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ad2852dae0541c1984aa9e959051d49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"acc9edb05e01430094c3a4f1ce56f8b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"848d475dab094f51afbf429d1e2802df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b05fdd593b749b78f3fba82fa939078":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f66f6b6d46ec472db840f9e6c46dd815":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab9e4059d2224a5c98869ba686a44ea7","IPY_MODEL_a6de46a8a18b4344892cb5d53f9601e6","IPY_MODEL_0e720df05be347eabb6331434ab29009"],"layout":"IPY_MODEL_cc05e3ec0b4d4e28a98ff2b4ba4dc917"}},"ab9e4059d2224a5c98869ba686a44ea7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fd1caba5325448ea1b4a273890c1410","placeholder":"‚Äã","style":"IPY_MODEL_741f91e293364837ac8300b270cadc03","value":"config.json:‚Äá100%"}},"a6de46a8a18b4344892cb5d53f9601e6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9580dacc12e24f43b0d719d0325a7561","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_57061dadcd494985a3f80d33de66aa79","value":385}},"0e720df05be347eabb6331434ab29009":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c6cded29d6141d3a7328e10d50cdb2a","placeholder":"‚Äã","style":"IPY_MODEL_0e852b49cb62423d9c04bffa3eda988f","value":"‚Äá385/385‚Äá[00:00&lt;00:00,‚Äá45.4kB/s]"}},"cc05e3ec0b4d4e28a98ff2b4ba4dc917":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fd1caba5325448ea1b4a273890c1410":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"741f91e293364837ac8300b270cadc03":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9580dacc12e24f43b0d719d0325a7561":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57061dadcd494985a3f80d33de66aa79":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c6cded29d6141d3a7328e10d50cdb2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e852b49cb62423d9c04bffa3eda988f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}