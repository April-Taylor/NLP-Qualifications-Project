{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/April-Taylor/NLP-Qualifications-Project/blob/main/Revised_Paired_MIMIC_NLP_ATaylor_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["# üîß Initial Project Setup (Run Once per Project)"],"metadata":{"id":"z-2Oq724hBu_"}},{"cell_type":"markdown","source":["This notebook sets up GitHub integration for a Google Colab project.\n","\n","It connects Colab to Google Drive and a GitHub repository, allowing work done\n","in Colab to be saved locally in Drive and versioned via GitHub. It also securely\n","loads any API keys or tokens needed (e.g., GitHub, WandB, UMLS, HuggingFace) using Colab's Secrets tool.\n","\n","\n","\n","**üìÅ Important:**\n","\n","\n","You must:\n","- Have a GitHub account and pre-created repository\n","- Use the same GitHub repo name in the USER INPUT section\n","- Place your working notebooks inside /notebooks subfolder of the Colab Google Drive. This ensures your work is versioned by Git and automatically pushed to GitHub at the end of each session.\n","- Create API tokens\n","\n","‚ö†Ô∏è To ensure autosave works correctly, always open this notebook directly from Google Drive ‚Äî not from ‚ÄúRecent,‚Äù ‚ÄúGitHub,‚Äù or an uploaded file.\n","\n","üîê Required Secrets:\n","- GitHubToken\n","- wandb (optional)\n","- UMLS (optional)\n","- HF_TOKEN (optional)\n","\n","‚ö†Ô∏è Make sure you have created these API tokens on their respective platforms and added them via the Colab Secrets UI (üîë icon on the left sidebar ‚Üí Add new secret). The GitHub Token is required for proper setup. The other secrets are optional depending on the project needs.\n","\n","\n","\n","üöÄ **To initiate a new project**\n","1.   Update the user inputs\n","2.   Run this notebook through RUN SETUP\n","\n","\n","üìå Note: This entire section should be run only once when starting a new project. It initializes the GitHub connection, sets up the folder structure, and loads your Colab secrets.\n","\n","For regular use:\n","\n","*   At the start of each new Colab session, run the \"Session Start: Sync from GitHub\" cell to mount Google Drive, load secrets, and pull any updates.\n","\n","*   At the end of your session, run the \"Push Changes to GitHub\" cell to save your work back to the repository."],"metadata":{"id":"F4ksWmzsgnVo"}},{"cell_type":"markdown","source":["## USER INPUT\n","\n"],"metadata":{"id":"nFt3BzLgg5Ji"}},{"cell_type":"code","source":["# Only update this section to reuse across projects\n","GITHUB_USER = \"April-Taylor\"\n","REPO_NAME = \"NLP-Qualifications-Project\"\n","USER_EMAIL = \"Taylor.april.dawn@gmail.com\"\n","USER_FULLNAME = \"April Taylor\"\n","REPO_DESCRIPTION = \"\"\"\n","This repository contains code, data, and deliverables related to NLP-based adverse event detection integrating structured glucose-insulin time series and unstructured clinical text. Designed for modular reuse across projects.\n","\"\"\"\n","DRIVE_BASE = \"MyDrive/ColabRepos\"  # ‚úÖ Adjust this if your folder structure is different\n","\n","from google.colab import drive, userdata\n","import os\n","\n"],"metadata":{"id":"sCQ73vsag2HB","executionInfo":{"status":"ok","timestamp":1752951018166,"user_tz":240,"elapsed":3,"user":{"displayName":"April Taylor","userId":"05794515634141358651"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## FUNCTIONALIZED SETUP\n","Project setup function - run only once per project"],"metadata":{"id":"ySAnDyomh73V"}},{"cell_type":"code","source":["def setup_colab_project(github_user, repo_name, user_email, user_fullname, repo_description, drive_base):\n","    project_path = f\"/content/drive/{drive_base}/{repo_name}\"\n","    repo_url = f\"https://github.com/{github_user}/{repo_name}.git\"\n","\n","    # Mount Google Drive\n","    if not os.path.ismount(\"/content/drive\"):\n","        drive.mount('/content/drive')\n","    os.makedirs(f\"/content/drive/{drive_base}\", exist_ok=True)\n","\n","    # Git identity setup\n","    !git config --global user.email \"{user_email}\"\n","    !git config --global user.name \"{user_fullname}\"\n","\n","    # Clone repo if not already in Drive\n","    if not os.path.exists(project_path):\n","        !git clone {repo_url} \"{project_path}\"\n","\n","    %cd \"{project_path}\"\n","\n","    # Load secrets securely\n","    os.environ['GITHUB_TOKEN'] = userdata.get('GitHubToken')\n","    os.environ['WANDB_API_KEY'] = userdata.get('wandb')\n","    os.environ['UMLS_API_KEY'] = userdata.get('UMLS')\n","    os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n","\n","    # Create folder structure\n","    for folder in [\"notebooks\", \"models\", \"data\", \"src\", \"outputs\"]:\n","        folder_path = os.path.join(project_path, folder)\n","        os.makedirs(folder_path, exist_ok=True)\n","        gitkeep = os.path.join(folder_path, \".gitkeep\")\n","        if not os.path.exists(gitkeep):\n","            open(gitkeep, \"w\").close()\n","\n","    # Add README if not present\n","    readme_path = os.path.join(project_path, \"README.md\")\n","    if not os.path.exists(readme_path):\n","        with open(readme_path, \"w\") as f:\n","            f.write(f\"\"\"# {repo_name}\n","\n","{repo_description}\n","\n","---\n","Maintained by **{github_user}**, 2025\n","\"\"\")\n","\n","    # Initial Git push\n","    push_url = f\"https://{github_user}:{os.environ['GITHUB_TOKEN']}@github.com/{github_user}/{repo_name}.git\"\n","    !git add .\n","    !git commit -m \"Initial setup from Colab\"\n","    !git push {push_url}\n","\n","    return project_path, github_user, repo_name"],"metadata":{"id":"WVGXL7vJhnfM","executionInfo":{"status":"ok","timestamp":1752951021711,"user_tz":240,"elapsed":19,"user":{"displayName":"April Taylor","userId":"05794515634141358651"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["## RUN SETUP (RUN ONCE: Set up project structure and sync with GitHub)\n","project_path, github_user, repo_name = setup_colab_project(\n","    GITHUB_USER, REPO_NAME, USER_EMAIL, USER_FULLNAME, REPO_DESCRIPTION, DRIVE_BASE\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Buchyz48iFFu","outputId":"6b915db1-6d2a-400b-9158-aff9010a7eb2","executionInfo":{"status":"ok","timestamp":1752951088598,"user_tz":240,"elapsed":63191,"user":{"displayName":"April Taylor","userId":"05794515634141358651"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/ColabRepos/NLP-Qualifications-Project\n","[main 1cb6859] Initial setup from Colab\n"," 1 file changed, 1 insertion(+), 2733 deletions(-)\n"," rewrite Revised_Paired_MIMIC_NLP_ATaylor_Project.ipynb (98%)\n","Enumerating objects: 5, done.\n","Counting objects: 100% (5/5), done.\n","Delta compression using up to 8 threads\n","Compressing objects: 100% (3/3), done.\n","Writing objects: 100% (3/3), 11.15 KiB | 1.39 MiB/s, done.\n","Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","To https://github.com/April-Taylor/NLP-Qualifications-Project.git\n","   84a58ec..1cb6859  main -> main\n"]}]},{"cell_type":"markdown","source":["# Daily Colab Session Setup (Run Every Day)\n","üì• Mount Drive & Load Secrets\n"],"metadata":{"id":"DhW89l6UAvz-"}},{"cell_type":"code","source":["from google.colab import drive, userdata\n","import os\n","\n","# Mount your Google Drive\n","drive.mount('/content/drive')\n","\n","# Navigate to your repo\n","%cd /content/drive/MyDrive/ColabRepos/NLP-Qualifications-Project\n","\n","# Git identity (needed for commits)\n","!git config --global user.email \"Taylor.april.dawn@gmail.com\"\n","!git config --global user.name \"April Taylor\"\n","\n","# üîê Load API tokens into environment\n","os.environ['GITHUB_TOKEN'] = userdata.get('GitHubToken')\n","os.environ['WANDB_API_KEY'] = userdata.get('wandb')\n","os.environ['UMLS_API_KEY'] = userdata.get('UMLS')\n","os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n"],"metadata":{"id":"IHobC6Zi_Xj1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# END-OF-SESSION PUSH TO GITHUB\n","Run this manually after your work session to sync with GitHub"],"metadata":{"id":"2g_TnazDiqAm"}},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","\n","# USER SETTINGS\n","GITHUB_USER = \"April-Taylor\"\n","REPO_NAME = \"NLP-Qualifications-Project\"\n","COMMIT_MESSAGE = \"End-of-session update from Colab\"\n","\n","# Authenticate using stored GitHub token\n","GITHUB_TOKEN = userdata.get(\"GitHubToken\")\n","push_url = f\"https://{GITHUB_USER}:{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git\"\n","\n","# Git push sequence\n","!git add .\n","!git commit -m \"{COMMIT_MESSAGE}\"\n","!git pull origin main --rebase\n","!git push \"{push_url}\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MIxOR9vtitm7","outputId":"2cff474f-d9e4-4c1d-8d33-885cda7c65e5","executionInfo":{"status":"ok","timestamp":1752950958255,"user_tz":240,"elapsed":751,"user":{"displayName":"April Taylor","userId":"05794515634141358651"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n"]}]},{"cell_type":"markdown","metadata":{"id":"EN9--XIYPdf-"},"source":["# Load Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vq7xIDgyu4N","outputId":"2ba9b804-38e1-4e0b-caab-699ed94d70c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n","Requirement already satisfied: spacy==3.4.4 in /usr/local/lib/python3.11/dist-packages (3.4.4)\n","Collecting scispacy==0.5.1\n","  Using cached scispacy-0.5.1-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (1.0.13)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (3.0.10)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (8.1.12)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (0.10.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (2.0.10)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (0.7.0)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (0.11.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (4.67.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (1.26.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (1.10.22)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (3.1.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (75.2.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (25.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.4.4) (3.5.0)\n","Requirement already satisfied: conllu in /usr/local/lib/python3.11/dist-packages (from scispacy==0.5.1) (6.0.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scispacy==0.5.1) (1.5.1)\n","Collecting nmslib>=1.7.3.6 (from scispacy==0.5.1)\n","  Using cached nmslib-2.1.1.tar.gz (188 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from scispacy==0.5.1) (1.6.1)\n","Requirement already satisfied: pysbd in /usr/local/lib/python3.11/dist-packages (from scispacy==0.5.1) (0.3.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.4.4) (1.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy==3.4.4) (4.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.4) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.4) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.4) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.4) (2025.7.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.4) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.4) (0.1.5)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.8.0,>=0.3.0->spacy==3.4.4) (8.2.1)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.4.4) (1.2.1)\n","Collecting pybind11<2.6.2 (from nmslib>=1.7.3.6->scispacy==0.5.1)\n","  Using cached pybind11-2.6.1-py2.py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from nmslib>=1.7.3.6->scispacy==0.5.1) (5.9.5)\n","Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.11/dist-packages (from pathy>=0.3.5->spacy==3.4.4) (0.1.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.3->scispacy==0.5.1) (1.15.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.3->scispacy==0.5.1) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy==3.4.4) (3.0.2)\n","Using cached scispacy-0.5.1-py3-none-any.whl (44 kB)\n","Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n","Building wheels for collected packages: nmslib\n","\u001b[33m  DEPRECATION: Building 'nmslib' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'nmslib'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n","\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m√ó\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for nmslib (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for nmslib\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for nmslib\n","Failed to build nmslib\n","\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (nmslib)\u001b[0m\u001b[31m\n","\u001b[0mCollecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_md-0.5.1.tar.gz\n","  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_md-0.5.1.tar.gz (120.2 MB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: spacy<3.5.0,>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from en_core_sci_md==0.5.1) (3.4.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (1.0.13)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (3.0.10)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (8.1.12)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (0.10.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (2.0.10)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (0.7.0)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (0.11.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (4.67.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (1.26.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (1.10.22)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (3.1.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (75.2.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (25.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (3.5.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (1.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (4.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (2025.7.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (0.1.5)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (8.2.1)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (1.2.1)\n","Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.11/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (0.1.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->en_core_sci_md==0.5.1) (3.0.2)\n","Collecting nmslib==2.1.1\n","  Using cached nmslib-2.1.1.tar.gz (188 kB)\n"]}],"source":["# Install Packages\n","!pip install --upgrade pip\n","!pip install spacy==3.4.4 scispacy==0.5.1\n","!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_md-0.5.1.tar.gz\n","!pip install nmslib==2.1.1 pybind11==2.6.1\n","\n","# Import Python Libraries\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import nltk\n","import re\n","\n","#  Machine Learning & Evaluation\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from joblib import Parallel, delayed\n","import torch\n","from torch.utils.data import Dataset\n","\n","# Transformers (Hugging Face)\n","from transformers import (\n","    BertTokenizerFast, BertForTokenClassification,\n","    Trainer, TrainingArguments,\n","    DataCollatorForTokenClassification\n",")\n","\n","# SpaCy + SciSpaCy\n","import spacy\n","import scispacy\n","from scispacy.umls_linking import UmlsEntityLinker\n","from spacy.pipeline import EntityRuler\n","from spacy import displacy\n","from pathlib import Path\n","from collections import defaultdict\n","\n","# W&B Setup\n","import wandb\n","wandb.init(\n","    project=\"clinical-umls-linking\",\n","    config={\"min_score\": 0.75},\n","    tags=[\"scispacy-md\", \"glucose-insulin\", \"umls-caching\"]\n",")\n","\n","# Load NLP Model with UMLS Linker\n","nlp = spacy.load(\"en_core_sci_md\")\n","nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n","\n","# Download NLTK Stopwords\n","nltk.download('stopwords')\n","nltk_stopwords = set(nltk.corpus.stopwords.words('english'))\n","\n","# Set Seeds for Reproducibility\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n"]},{"cell_type":"markdown","metadata":{"id":"d2I9iC4JMKi9"},"source":["# Data Loading"]},{"cell_type":"markdown","metadata":{"id":"MmI1k9iuPv9k"},"source":["## Load MIMIC-III curated glucose-insulin paired data"]},{"cell_type":"markdown","metadata":{"id":"M2IXwh0OOcIA"},"source":["The project uses the glucose_insulin_pair.csv dataset from the curated glucose-insulin files, available on PhysioNet (https://physionet.org/content/glucose-management-mimic/1.0.1/Notebooks/#files-panel)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qoDIupSnmehC"},"outputs":[],"source":["# Load the data into a dataframe\n","insulin_file_path = \"/content/drive/MyDrive/glucose_insulin_pair.csv\"\n","columns_needed = ['SUBJECT_ID', 'HADM_ID', 'GLC', 'GLC_AL', 'GLCSOURCE','EVENT', 'INSULINTYPE','INPUT', 'INPUT_HRS', 'INFXSTOP', 'TIMER']\n","insulin_data = pd.read_csv(insulin_file_path, usecols=columns_needed)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-d2UE6NREyHu"},"outputs":[],"source":["insulin_data.shape"]},{"cell_type":"markdown","metadata":{"id":"gvlXVp5-OLPx"},"source":["## Load MIMIC-III v1.4 notes data"]},{"cell_type":"markdown","metadata":{"id":"8Ay7QlklDgE7"},"source":["The project uses the NOTEEVENTS.csv dataset from MIMIC-III, available on PhysioNet (https://physionet.org/content/mimiciii/1.4/). These notes align with the same MIMIC-III data from which the curated glucose_insulin_pair data is derived."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DAqbD60bUvlC"},"outputs":[],"source":["# Load the data into a dataframe\n","notes_file_path = \"/content/drive/My Drive/NOTEEVENTS.csv\"\n","notes_cols = ['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'CATEGORY', 'TEXT']\n","noteevents = pd.read_csv(notes_file_path, usecols=notes_cols, low_memory=False)"]},{"cell_type":"markdown","source":["# Inspect Data"],"metadata":{"id":"WJHaXzB9kLQf"}},{"cell_type":"markdown","metadata":{"id":"STxMHL38ANYP"},"source":["## Inspect the Glucose_Insulin_Pair dataset"]},{"cell_type":"markdown","metadata":{"id":"bD8RgSnfEWzQ"},"source":["**Description of fields:**\n","*   **SUBJECT_ID:** It is the unique identifier for an individual patient.\n","*  **HADM_ID:**Represents a single patient‚Äôs admission to the hospital.\n","*   **ICUSTAY_ID:** Unique identifier for a single patient‚Äôs admission to the ICU.\n","*   **LOS_ICU_days:** Length of stay in days.\n","*   **first_ICU_stay:** True if it is the first admission to the ICU for a hospital admission.\n","*   **TIMER:** Gathers the timestamps for either the STARTTIME for a single insulin input or the GLCTIMER for a single glucose reading. It is used to order chronologically the events along a hospital admission.\n","*   **STARTTIME:** Timestamp that depicts when the administration of an insulin event started or when a new infusion rate was indicated.\n","*   **INPUT:** Dose for a single bolus of insulin in U.\n","*   **INPUT_HRS:** Insulin infusion rate in units/hr.\n","*   **ENDTIME:** Timestamp that specifies when an insulin input stopped, or an infusion rate changed.\n","*   **INSULINTYPE:** Acting type of insulin: short, intermediate, or long.\n","*   **EVENT:** Specifies whether the bolus of insulin was subcutaneous (BOLUS_INYECTION), or intravenous (BOLUS_PUSH), or if the insulin was infused (INFUSION).\n","*   **INFXSTOP:** Indicates when an infusion of insulin was discontinued. A value equal to 1 indicates when an infusion was discontinued, otherwise (entries equal to 0) this column indicates that the associated infusion started or the rate of infusion was modified.\n","*   **GLCTIMER:** Timestamp that depicts when a glycemic check was done.\n","*   **GLC:** Glycemia value in mg/dL.\n","*   **GLCSOURCE:** Reading method for a glycemic check: fingerstick (FINGERSTICK) or lab analyzer (BLOOD).\n","\n","Specific to glucose_insulin_pair.csv\n","*   **GLCTIMER_AL:** Timestamp that depicts when a glycemic check was done for a paired glucose reading. This value should match with the timestamp in GLCTIMER of a preceding glucose reading according to the rule applied for this pairing case.\n","*   **GLC_AL:** Glycemia value in mg/dL for a paired glucose reading with a single insulin input. This value should match with the value in GLC of a preceding glucose reading according to the rule applied for this pairing case.\n","*   **GLCSOURCE_AL:** Reading method for a glycemic check that was paired with an insulin input. This value should match with the GLCSOURCE value of a preceding glucose reading according to the rule applied for this pairing case.\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zgTZjQb6nM_O"},"outputs":[],"source":["# Inspect the Glucose_Insulin_Pair dataset\n","print(insulin_data.head(10))\n","print(insulin_data.info())\n","print(insulin_data.describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"acEIVAKRoXgY"},"outputs":[],"source":["# Inspect initial dataset sizes\n","print(f\"Original insulin_data size: {insulin_data.shape[0]} rows\")\n","print(f\"Original noteevents size: {noteevents.shape[0]} rows\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5SvSFDzhqtb"},"outputs":[],"source":["print(f\"Missing TIMER in insulin_data: {insulin_data['TIMER'].isna().sum()} out of {len(insulin_data)}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nMZ0kIYwrqD4"},"source":["# PreProcess Glucose_Insulin Data"]},{"cell_type":"markdown","metadata":{"id":"DWP84ZoY3tOa"},"source":["## Filter and format glucose/insulin data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5oyWGWUMpX6z"},"outputs":[],"source":["# Convert HADM_ID to int\n","insulin_data['HADM_ID'] = insulin_data['HADM_ID'].fillna(-1).astype(int)\n","\n","# Ensure TIMER is datetime and timezone-naive for both insulin and glucose datasets\n","insulin_data['TIMER'] = pd.to_datetime(insulin_data['TIMER'], errors='coerce').dt.tz_localize(None)\n","if insulin_data['TIMER'].isna().any():\n","    print(\"Invalid TIMER values detected.\")\n","\n","\n","# Verify that all TIMER values are properly formatted\n","assert insulin_data['TIMER'].notna().all(), \"TIMER in insulin_data contains NaT!\"\n","\n","\n","# Remove fully duplicate rows (all columns identical)\n","insulin_data = insulin_data.drop_duplicates()\n","\n","# Sort the insulin_data by TIMER for merging\n","insulin_data = insulin_data.sort_values(by='TIMER').reset_index(drop=True)\n","\n","# Verify sorting\n","assert insulin_data['TIMER'].is_monotonic_increasing, \"TIMER column is not sorted!\"\n","\n","# Final check of the data structure\n","print(\"Final insulin_data structure:\")\n","print(insulin_data.info())\n","\n","# Sample to verify the structure\n","print(\"Sample of data after removing duplicates:\")\n","print(insulin_data.head())\n"]},{"cell_type":"markdown","metadata":{"id":"jZB3fPsZfeVZ"},"source":["### Create a combined column for all glucose values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNRRMwq9fc2v"},"outputs":[],"source":["# Separate rows with GLC_AL values\n","glc_al_data = insulin_data[~insulin_data['GLC_AL'].isna()].copy()\n","\n","# Separate rows with GLC values\n","glc_only_data = insulin_data[~insulin_data['GLC'].isna()].copy()\n","\n","import pandas as pd\n","\n","# Sort both datasets by TIMER\n","glc_only_data = glc_only_data.sort_values('TIMER')\n","glc_al_data = glc_al_data.sort_values('TIMER')\n","\n","# Create a temporary merge to find matches within the 2-hour window\n","merged_within_2hr = pd.merge_asof(\n","    glc_only_data,\n","    glc_al_data[['TIMER']],  # Only use TIMER column from glc_al_data for matching\n","    on='TIMER',\n","    tolerance=pd.Timedelta(hours=2),  # Define the 2-hour window\n","    direction='nearest'  # Allow matching in both directions\n",")\n","\n","# Reset index of glc_only_data and merged_within_2hr before filtering\n","glc_only_data = glc_only_data.reset_index(drop=True)\n","merged_within_2hr = merged_within_2hr.reset_index(drop=True)\n","\n","\n","# Filter out rows from glc_only_data that have a match in glc_al_data within 2 hours\n","filtered_glc_only_data = glc_only_data[merged_within_2hr['TIMER'].isna()]\n","\n","# Combine GLC_AL and filtered GLC data\n","GLC_ALL = pd.concat([glc_al_data, filtered_glc_only_data], ignore_index=True)\n","\n","\n","# Add GLC_ALL column to insulin_data for consistency\n","insulin_data['GLC_ALL'] = insulin_data['GLC_AL'].combine_first(insulin_data['GLC'])\n","\n","# Verify the structure of insulin_data\n","print(\"Updated insulin_data with GLC_ALL:\")\n","print(insulin_data.info())\n","print(insulin_data[['TIMER', 'GLC', 'GLC_AL', 'GLC_ALL']].head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cn3Zf_WmUVK"},"outputs":[],"source":["# Aggregate insulin_data at SUBJECT_ID, HADM_ID level\n","insulin_aggregated = insulin_data.groupby(['SUBJECT_ID', 'HADM_ID']).agg(\n","    TotalBolus=('INPUT', 'sum'),                  # Total bolus insulin administered\n","    AvgBolus=('INPUT', 'mean'),                  # Average bolus insulin per event\n","    MaxBolus=('INPUT', 'max'),                   # Max bolus insulin\n","    TotalInfusion=('INPUT_HRS', 'sum'),          # Total infusion hours\n","    AvgInfusionRate=('INPUT_HRS', 'mean'),       # Average infusion rate\n","    MaxGlucose=('GLC_ALL', 'max'),                   # Maximum glucose level\n","    MinGlucose=('GLC_ALL', 'min'),                   # Minimum glucose level\n","    AvgGlucose=('GLC_ALL', 'mean'),                  # Average glucose level\n","    InfusionStops=('INFXSTOP', 'sum')            # Count of infusion stops\n",").reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCeX7GmFocl_"},"outputs":[],"source":["print(insulin_aggregated.head())"]},{"cell_type":"markdown","metadata":{"id":"z5rP3iRL8Vfk"},"source":["## Categorize Glucose_Insulin data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6llLS2tvZvI"},"outputs":[],"source":["# Categorize insulin delivery types\n","def categorize_insulin_delivery(row):\n","    if pd.isna(row['INPUT']):  # No bolus given\n","        if pd.isna(row['INPUT_HRS']):  # No infusion rate recorded\n","            return \"No insulin adjustment\"\n","    elif isinstance(row['EVENT'], str) and (\"BOLUS_INYECTION\" in row['EVENT'] or \"BOLUS_PUSH\" in row['EVENT']):\n","        return \"Bolus\"\n","    elif isinstance(row['EVENT'], str) and \"INFUSION\" in row['EVENT']:\n","        return \"Infusion\"\n","    return \"Unknown\"  # Fallback case\n","\n","# Apply the function to categorize insulin delivery\n","insulin_data['InsulinDeliveryCategory'] = insulin_data.apply(categorize_insulin_delivery, axis=1)\n","\n","# Categorize glucose levels\n","def categorize_glucose(value):\n","    if pd.isna(value):  # Handle NaN case\n","        return None\n","    elif value < 70:\n","        return \"Hypoglycemic\"\n","    elif value > 180:\n","        return \"Hyperglycemic\"\n","    else:\n","        return \"Euglycemic\"\n","\n","# Apply glucose categorization to the GLC_ALL column\n","insulin_data['GlucoseCategory'] = insulin_data['GLC_ALL'].apply(categorize_glucose)\n","\n","# Verify updated categories\n","print(\"\\nUnique Insulin Delivery Categories in categorized_data after refined processing:\")\n","print(insulin_data['InsulinDeliveryCategory'].unique())\n","\n","print(\"\\nUnique Glucose Categories in insulin_data after refined processing:\")\n","print(insulin_data['GlucoseCategory'].unique())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHGFJWoi77e_"},"outputs":[],"source":["# Count the number of glucose events by category\n","glucose_category_counts = insulin_data['GlucoseCategory'].value_counts()\n","\n","# Bar Plot for Glucose Category Distribution with AGP Colors\n","plt.figure(figsize=(10, 6))\n","\n","# Define AGP colors for each category\n","agp_colors = {\n","    'Hypoglycemic': 'red',\n","    'Euglycemic': 'green',\n","    'Hyperglycemic': 'orange'\n","}\n","\n","# Get the colors for the bars in the correct order\n","bar_colors = [agp_colors[category] for category in glucose_category_counts.index]\n","\n","# Create the bar plot with AGP colors\n","bars = sns.barplot(\n","    x=glucose_category_counts.index,\n","    y=glucose_category_counts.values,\n","    palette=bar_colors  # Use the AGP colors\n",")\n","\n","plt.title('Distribution of Glucose Categories')\n","plt.xlabel('Glucose Category')\n","plt.ylabel('Count')\n","plt.xticks(rotation=45)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Asp9ZO4qNGpL"},"outputs":[],"source":["# Count occurrences of each label in 'GlucoseCategory'\n","glucose_category_counts = insulin_data['GlucoseCategory'].value_counts()\n","print(\"Glucose Category Counts:\\n\", glucose_category_counts)\n","\n","# Count occurrences of each label in 'InsulinDeliveryCategory'\n","insulin_delivery_counts = insulin_data['InsulinDeliveryCategory'].value_counts()\n","print(\"\\nInsulin Delivery Category Counts:\\n\", insulin_delivery_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QdsmgevJ-Z66"},"outputs":[],"source":["# Frequency of insulin categories\n","plt.figure(figsize=(10, 6))\n","sns.countplot(data=insulin_data, x='InsulinDeliveryCategory', order=insulin_data['InsulinDeliveryCategory'].value_counts().index)\n","plt.title('Frequency of Insulin Delivery Categories')\n","plt.xlabel('Insulin Delivery Category')\n","plt.ylabel('Count')\n","plt.xticks(rotation=45)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"qA50KWZsgDo2"},"source":["# Exploratory Data Analysis (EDA)"]},{"cell_type":"markdown","metadata":{"id":"B8CVZuDWAgt7"},"source":["##Distributions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWb0zjKiJ_rX"},"outputs":[],"source":["# Statistical summaries for Glucose, Bolus, and Infusion\n","summary_stats = insulin_data[['GLC_ALL', 'INPUT', 'INPUT_HRS']].rename(columns={\n","    'GLC_ALL': 'Glucose',\n","    'INPUT': 'Bolus',\n","    'INPUT_HRS': 'Infusion'\n","}).describe()\n","\n","print(\"Statistical Summary:\")\n","print(summary_stats)\n"]},{"source":["# Consolidated Distribution Plot\n","def plot_distribution(data, column, title, xlabel, bins=30, color='blue'):\n","    plt.figure(figsize=(10, 6))\n","    sns.histplot(data[column], bins=bins, kde=True, color=color)\n","    plt.title(title)\n","    plt.xlabel(xlabel)\n","    plt.ylabel('Frequency')\n","\n","# Plot Glucose Levels\n","plot_distribution(insulin_data, 'GLC_ALL', 'Distribution of Glucose Levels', 'Glucose (mg/dL)', color='red')\n","plt.xticks(range(0, int(insulin_data['GLC_ALL'].max()) + 100, 100))\n","plt.show()\n","\n","# Plot Bolus Insulin Doses\n","plot_distribution(insulin_data, 'INPUT', 'Distribution of Bolus Insulin Doses', 'Bolus Insulin (units)', color='orange')\n","plt.show()\n","\n","# Plot Infusion Rates\n","plot_distribution(insulin_data, 'INPUT_HRS', 'Distribution of Infusion Rates', 'Infusion Rate (units/hr)', color='blue')\n","plt.show()"],"cell_type":"code","metadata":{"id":"RN3leYlNk00z"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"svIT8S50BdYQ"},"outputs":[],"source":["# Select relevant columns for plotting\n","boxplot_data = insulin_data[['GLC_ALL', 'INPUT', 'INPUT_HRS']].rename(columns={\n","    'GLC_ALL': 'Glucose',\n","    'INPUT': 'Bolus',\n","    'INPUT_HRS': 'Infusion'\n","})\n","\n","# Define the column names and titles for the plots\n","columns = ['Glucose', 'Bolus', 'Infusion']\n","titles = ['Distribution of Glucose Levels', 'Distribution of Bolus Insulin Doses', 'Distribution of Infusion Rates']\n","\n","# Create individual boxplots\n","for i, col in enumerate(columns):\n","    plt.figure(figsize=(8, 6))\n","    sns.boxplot(data=boxplot_data, y=col, color='skyblue', width=0.5)\n","\n","    # Add plot labels and title\n","    plt.title(titles[i], fontsize=16)\n","    plt.ylabel(col, fontsize=14)\n","    plt.xticks([])  # Remove x-ticks for simplicity\n","    plt.grid(axis='y', linestyle='--', alpha=0.7)\n","\n","    # Adjust y-axis ticks for better granularity\n","    if col == 'Bolus':  # For Bolus, set y-ticks to every 20 units\n","        plt.yticks(range(0, int(boxplot_data[col].max() + 1), 20))\n","    elif col != 'Glucose':  # For Infusion, finer granularity (1 unit steps)\n","        plt.yticks(range(0, int(boxplot_data[col].max() + 1), 1))\n","\n","    # Display the plot\n","    plt.tight_layout()\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFlK27E60Qb_"},"outputs":[],"source":["hypoglycemic_cases = insulin_data[insulin_data['GLC_ALL'] < 70]\n","print(hypoglycemic_cases)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tkX9SAZd_oig"},"outputs":[],"source":["# Count of each insulin category\n","insulin_category_counts = insulin_data['InsulinDeliveryCategory'].value_counts()\n","print(\"Insulin Category Counts:\\n\", insulin_category_counts)\n","\n","# Count of each glucose category\n","glucose_category_counts = insulin_data['GlucoseCategory'].value_counts()\n","print(\"\\nGlucose Category Counts:\\n\", glucose_category_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmmi3RTNhgAd"},"outputs":[],"source":["# Group data by GlucoseCategory, InsulinDeliveryCategory, and INFXSTOP\n","grouped_data = insulin_data.groupby(['GlucoseCategory', 'InsulinDeliveryCategory']).size().reset_index(name='Count')\n","\n","# Pivot table for heatmap (optional to include INFXSTOP as additional dimension)\n","pivot_data = grouped_data.pivot_table(index='GlucoseCategory', columns='InsulinDeliveryCategory', values='Count', aggfunc='sum')\n","\n","plt.figure(figsize=(10, 6))\n","sns.heatmap(\n","    pivot_data,\n","    annot=True,\n","    fmt=\".0f\",\n","    cmap=\"YlGnBu\",\n","    linewidths=0.5,\n","    cbar_kws={\"label\": \"Event Count\"}\n",")\n","plt.title('Insulin Delivery by Glucose Category')\n","plt.xlabel('Insulin Delivery Method')\n","plt.ylabel('Glucose Category')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RyhgcLNVuU47"},"outputs":[],"source":["# Group by SUBJECT_ID and calculate metrics\n","grouped_data = insulin_data.groupby('SUBJECT_ID').agg({\n","    'GLC_ALL': 'mean',  # Average glucose level\n","    'INPUT': 'mean',  # Average insulin dose per event\n","    'GlucoseCategory': 'count'  # Number of glucose readings\n","}).rename(columns={'GLC_ALL': 'AvgGlucose', 'INPUT': 'AvgInsulinPerEvent', 'GlucoseCategory': 'NumReadings'})\n","\n","# Full scatterplot\n","plt.figure(figsize=(12, 8))\n","sns.scatterplot(\n","    data=grouped_data,\n","    x='AvgInsulinPerEvent',\n","    y='AvgGlucose',\n","    hue='NumReadings',\n","    palette='coolwarm',\n","    size='NumReadings',\n","    sizes=(20, 200)\n",")\n","plt.axhline(y=70, color='red', linestyle='--', label='Hypoglycemia (<70 mg/dL)')\n","plt.axhline(y=180, color='orange', linestyle='--', label='Hyperglycemia (>180 mg/dL)')\n","plt.title('Relationship between Avg. Insulin Dose per Event and Avg. Glucose Levels (Full Range)', fontsize=16)\n","plt.xlabel('Avg. Insulin Dose per Event (units)', fontsize=14)\n","plt.ylabel('Average Glucose Level (mg/dL)', fontsize=14)\n","plt.legend(title=\"Num. of Readings\", loc='upper right', fontsize=10)\n","plt.grid(True)\n","plt.show()\n","\n","# Filter for granular zoom: 1‚Äì20 units of insulin\n","filtered_zoom_data = grouped_data[(grouped_data['AvgInsulinPerEvent'] >= 1) & (grouped_data['AvgInsulinPerEvent'] <= 20)]\n","\n","# Zoomed scatterplot\n","plt.figure(figsize=(12, 8))\n","sns.scatterplot(\n","    data=filtered_zoom_data,\n","    x='AvgInsulinPerEvent',\n","    y='AvgGlucose',\n","    hue='NumReadings',\n","    palette='coolwarm',\n","    size='NumReadings',\n","    sizes=(20, 200)\n",")\n","plt.axhline(y=70, color='red', linestyle='--', label='Hypoglycemia (<70 mg/dL)')\n","plt.axhline(y=180, color='orange', linestyle='--', label='Hyperglycemia (>180 mg/dL)')\n","plt.title('Relationship between Avg. Insulin Dose per Event and Avg. Glucose Levels (1‚Äì20 Units)', fontsize=16)\n","plt.xlabel('Avg. Insulin Dose per Event (units)', fontsize=14)\n","plt.ylabel('Average Glucose Level (mg/dL)', fontsize=14)\n","plt.xticks(ticks=range(1, 21, 1))  # Sets x-axis ticks from 1 to 20 with step of 1\n","plt.legend(title=\"Num. of Readings\", loc='upper right', fontsize=10)\n","plt.grid(True)\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"a2xx2gBtBMn_"},"source":["##Time Series Comparisons"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVijGTz1BQtH"},"outputs":[],"source":["import random\n","import matplotlib.pyplot as plt\n","\n","random.seed(42)\n","\n","def plot_time_series(insulin_data, subject_ids, random_seed=42):\n","    \"\"\"Plots time series for specified subjects, highlighting insulin values with black labels and red dots.\"\"\"\n","\n","    for subject_id in subject_ids:\n","        subject_data = insulin_data[insulin_data['SUBJECT_ID'] == subject_id]\n","        if not subject_data.empty:\n","            plt.figure(figsize=(15, 4))\n","\n","            # Plot glucose levels\n","            plt.plot(subject_data['TIMER'], subject_data['GLC_ALL'], label='Glucose (mg/dL)', color='blue')\n","\n","            # Highlight insulin data points as red dots\n","            insulin_data_points = subject_data[~subject_data['INPUT'].isna()]\n","            plt.scatter(\n","                insulin_data_points['TIMER'],\n","                insulin_data_points['INPUT'],\n","                color='red',\n","                label='Bolus Doses (units)',\n","                marker='o',\n","                s=50\n","            )\n","\n","            # Add data labels for insulin points (as whole numbers and black text)\n","            for _, row in insulin_data_points.iterrows():\n","                plt.text(\n","                    row['TIMER'],\n","                    row['INPUT'] + 10,  # Offset the label slightly above the point\n","                    f\"{int(row['INPUT'])}\",  # Convert to whole number\n","                    fontsize=9,\n","                    color='black',\n","                    ha='center'\n","                )\n","\n","            plt.xlabel('Date')\n","            plt.ylabel('Values')\n","            plt.title(f'Time Series for Subject {subject_id}')\n","            plt.xticks(rotation=45)\n","            plt.legend()\n","            plt.tight_layout()\n","            plt.ylim(0, 400)\n","            plt.show()\n","\n","# Get three random subject IDs from the data\n","num_subjects_to_plot = 3\n","subject_ids = random.sample(list(insulin_data['SUBJECT_ID'].unique()), num_subjects_to_plot)\n","\n","# Plot the time series for those subjects, one plot per subject\n","plot_time_series(insulin_data, subject_ids, random_seed=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0NYP9MLz_16L"},"outputs":[],"source":["# Inspect raw insulin and glucose values for the selected patient\n","patient_id =55642  # Replace with the patient ID from your plot\n","raw_patient_data = insulin_data[insulin_data['SUBJECT_ID'] == patient_id]\n","\n","# Display raw insulin and glucose data\n","print(\"Raw Data for Patient:\")\n","print(raw_patient_data[['TIMER', 'GLC_ALL', 'INPUT', 'INPUT_HRS', 'InsulinDeliveryCategory']])\n","\n","# Check unique values for INPUT and INPUT_HRS\n","print(\"\\nUnique Bolus Insulin Doses (INPUT):\")\n","print(raw_patient_data['INPUT'].unique())\n","\n","print(\"\\nUnique Infusion Rates (INPUT_HRS):\")\n","print(raw_patient_data['INPUT_HRS'].unique())\n","\n","# Check if there are duplicates or overlapping data\n","duplicates = raw_patient_data.duplicated(subset=['TIMER', 'INPUT', 'INPUT_HRS'])\n","print(f\"\\nNumber of Duplicate Entries for Patient {patient_id}: {duplicates.sum()}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RXm1LFkF_as"},"outputs":[],"source":["!pip install adjustText\n","from adjustText import adjust_text\n","\n","\n","# Ensure TIMER is a datetime object\n","insulin_data['TIMER'] = pd.to_datetime(insulin_data['TIMER'], errors='coerce')\n","\n","# Extract Hour and Day from TIMER\n","insulin_data['Hour'] = insulin_data['TIMER'].dt.hour\n","insulin_data['Day'] = insulin_data['TIMER'].dt.date\n","\n","# Group by Hour and calculate mean values for Glucose, Bolus Insulin, and Infusion Rate\n","hourly_avg = insulin_data.groupby('Hour').agg({\n","    'GLC_ALL': 'mean',  # Average Glucose Level\n","    'INPUT': 'mean',         # Average Bolus Insulin\n","    'INPUT_HRS': 'mean'      # Average Infusion Rate\n","}).rename(columns={\n","    'GLC_ALL': 'Glucose (mg/dL)',\n","    'INPUT': 'Bolus (units)',\n","    'INPUT_HRS': 'Infusion (units/hr)'\n","}).reset_index()\n","\n","# Plot the hourly trends\n","plt.figure(figsize=(14, 8))\n","\n","# Initialize the list for dynamic text annotations\n","texts = []\n","\n","# Plot Glucose Levels\n","plt.plot(hourly_avg['Hour'], hourly_avg['Glucose (mg/dL)'], marker='o', label='Glucose (mg/dL)', color='blue', linestyle='-')\n","for _, row in hourly_avg.iterrows():\n","    texts.append(\n","        plt.text(\n","            row['Hour'], row['Glucose (mg/dL)'],  # Add text dynamically\n","            f\"{round(row['Glucose (mg/dL)'], 1)}\",\n","            fontsize=9, color='blue', ha='center'\n","        )\n","    )\n","\n","# Plot Bolus Insulin Levels\n","plt.plot(hourly_avg['Hour'], hourly_avg['Bolus (units)'], marker='x', label='Bolus (units)', color='red', linestyle='--')\n","for _, row in hourly_avg.iterrows():\n","    texts.append(\n","        plt.text(\n","            row['Hour'], row['Bolus (units)'],  # Add text dynamically\n","            f\"{round(row['Bolus (units)'], 1)}\",\n","            fontsize=9, color='red', ha='center'\n","        )\n","    )\n","\n","# Plot Infusion Rates\n","plt.plot(hourly_avg['Hour'], hourly_avg['Infusion (units/hr)'], marker='s', label='Infusion (units/hr)', color='green', linestyle='-.')\n","for _, row in hourly_avg.iterrows():\n","    texts.append(\n","        plt.text(\n","            row['Hour'], row['Infusion (units/hr)'],  # Add text dynamically\n","            f\"{round(row['Infusion (units/hr)'], 1)}\",\n","            fontsize=9, color='green', ha='center'\n","        )\n","    )\n","\n","# Adjust overlapping text labels\n","adjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray'))\n","\n","# Customize plot\n","plt.title('Hourly Trends in Glucose and Insulin Levels (1-24 Hours)', fontsize=16)\n","plt.xlabel('Hour of Day (0-23)', fontsize=14)\n","plt.ylabel('Average Levels', fontsize=14)\n","\n","# Set y-axis ticks and limits\n","y_min = 0  # Minimum value for y-axis\n","y_max = int(hourly_avg[['Glucose (mg/dL)', 'Bolus (units)', 'Infusion (units/hr)']].max().max() + 20)\n","plt.yticks(np.arange(y_min, y_max + 1, 20))  # Intervals of 20\n","\n","# Configure x-axis ticks for hourly scale\n","plt.xticks(ticks=np.arange(0, 24), labels=np.arange(0, 24), fontsize=12)\n","plt.grid(True, linestyle='--', alpha=0.6)\n","\n","# Adjust legend placement\n","plt.legend(title=\"Legend\", fontsize=12, loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n","\n","# Adjust layout to accommodate legend\n","plt.tight_layout(rect=[0, 0.05, 1, 1])\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2BlA8wAcc2J"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from adjustText import adjust_text\n","import random\n","\n","random_seed = 42\n","random.seed(random_seed)\n","\n","# Filter for patients with both insulin and glucose values\n","pairwise_distances_argmin_min_data = insulin_data[\n","    (insulin_data['GLC_ALL'].notna()) &  # Glucose value present\n","    (\n","       insulin_data['INPUT'].notna() |  # Bolus insulin\n","        insulin_data['INPUT_HRS'].notna() |  # Infusion rate\n","        (insulin_data['INFXSTOP'] == 1)  # Infusion stop\n","    )\n","]\n","\n","# Get a list of unique patient IDs from paired data\n","valid_patient_ids = insulin_data['SUBJECT_ID'].unique()\n","\n","# Select 3 random patient IDs from valid ones\n","if len(valid_patient_ids) < 3:\n","    print(\"Not enough patients with both insulin and glucose values for plotting.\")\n","else:\n","    random_patient_ids = random.sample(list(valid_patient_ids), 3)\n","\n","    # Iterate over each random patient ID and create individual plots\n","    for patient_id in random_patient_ids:\n","        # Filter data for this patient\n","        patient_data = insulin_data[insulin_data['SUBJECT_ID'] == patient_id].copy()\n","\n","        # Ensure TIMER is sorted\n","        patient_data = patient_data.sort_values(by='TIMER')\n","\n","        # Check if patient data is empty after filtering\n","        if patient_data.empty:\n","            print(f\"No valid data for patient ID {patient_id}.\")\n","            continue\n","\n","        # Create text annotations list\n","        texts = []\n","\n","        # Initialize the figure\n","        plt.figure(figsize=(14, 8))\n","\n","        # Plot Glucose Levels\n","        glucose_data = patient_data[patient_data['GLC_ALL'] > 0]\n","        plt.plot(\n","            glucose_data['TIMER'],\n","            glucose_data['GLC_ALL'],\n","            color='blue',\n","            label='Glucose Levels (mg/dL)',\n","            marker='o',\n","            linestyle='-',\n","            alpha=0.7\n","        )\n","        for _, row in glucose_data.iterrows():\n","            texts.append(\n","                plt.text(\n","                    row['TIMER'], row['GLC_ALL'] + 5,  # Offset upward\n","                    f\"{int(row['GLC_ALL'])} mg/dL\",\n","                    fontsize=10,\n","                    color='blue',\n","                    ha='center'\n","                )\n","            )\n","\n","        # Scatterplot for Bolus Insulin\n","        bolus_data = patient_data[patient_data['INPUT'] > 0]\n","        plt.scatter(\n","            bolus_data['TIMER'],\n","            bolus_data['INPUT'],\n","            color='red',\n","            label='Bolus Insulin (Units)',\n","            alpha=0.7,\n","            marker='x'\n","        )\n","        for _, row in bolus_data.iterrows():\n","            texts.append(\n","                plt.text(\n","                    row['TIMER'], row['INPUT'] + 5,  # Offset upward\n","                    f\"{int(row['INPUT'])} units\",\n","                    fontsize=10,\n","                    color='red',\n","                    ha='center'\n","                )\n","            )\n","\n","        # Scatterplot for Infusion Rates\n","        infusion_data = patient_data[patient_data['INPUT_HRS'] > 0]\n","        plt.scatter(\n","            infusion_data['TIMER'],\n","            infusion_data['INPUT_HRS'],\n","            color='green',\n","            label='Infusion Rate (Units/hr)',\n","            alpha=0.7,\n","            marker='s'\n","        )\n","        for _, row in infusion_data.iterrows():\n","            texts.append(\n","                plt.text(\n","                    row['TIMER'], row['INPUT_HRS'] + 5,  # Offset upward\n","                    f\"{int(row['INPUT_HRS'])} units/hr\",\n","                    fontsize=10,\n","                    color='green',\n","                    ha='center'\n","                )\n","            )\n","\n","        # Scatterplot for Infusion Stops\n","        infusion_stop_data = patient_data[patient_data['INFXSTOP'] == 1]\n","        plt.scatter(\n","            infusion_stop_data['TIMER'],\n","            infusion_stop_data['INPUT_HRS'],\n","            color='purple',\n","            label='Infusion Stopped',\n","            alpha=0.9,\n","            marker='P',\n","            s=150\n","        )\n","        for _, row in infusion_stop_data.iterrows():\n","            texts.append(\n","                plt.text(\n","                    row['TIMER'], row['INPUT_HRS'] + 5,  # Offset upward\n","                    \"Stopped\",\n","                    fontsize=10,\n","                    color='purple',\n","                    ha='center'\n","                )\n","            )\n","\n","        # Adjust overlapping text labels\n","        adjust_text(texts, arrowprops=dict(arrowstyle='-', color='black'))\n","\n","        # Customize the plot\n","        plt.title(f'Overlay of Glucose and Insulin Trends for Patient ID {patient_id}', fontsize=16)\n","        plt.xlabel('Time', fontsize=14)\n","        plt.ylabel('Values (Glucose Levels and Insulin Doses)', fontsize=14)\n","        plt.legend(fontsize=12)\n","        plt.ylim(0)\n","        plt.grid(True)\n","        plt.xticks(rotation=45)\n","\n","        # Show the plot\n","        plt.tight_layout()\n","        plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"HYIRkJJnAt9U"},"source":["##Correlations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QayTMDyaKagW"},"outputs":[],"source":["# Correlation matrix\n","correlation_matrix = insulin_data[['GLC_ALL', 'INPUT', 'INPUT_HRS']].rename(columns={\n","    'GLC_ALL': 'Glucose',\n","    'INPUT': 'Bolus',\n","    'INPUT_HRS': 'Infusion'\n","}).corr(method='pearson')\n","\n","print(\"Correlation Matrix:\")\n","print(correlation_matrix)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-tk5PG3lIUcT"},"outputs":[],"source":["# Select relevant columns for correlation analysis\n","correlation_data = insulin_data[['GLC_ALL', 'INPUT', 'INPUT_HRS']].rename(columns={\n","    'GLC_ALL': 'Glucose',\n","    'INPUT': 'Bolus',\n","    'INPUT_HRS': 'Infusion'\n","})\n","\n","# Calculate the correlation matrix\n","correlation_matrix = correlation_data.corr()\n","\n","# Plot the correlation heatmap\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(\n","    correlation_matrix,\n","    annot=True,  # Display correlation coefficients\n","    cmap='coolwarm',  # Color map for visualization\n","    fmt=\".2f\",  # Format the correlation coefficients\n","    linewidths=0.5,  # Add space between cells\n","    cbar_kws={\"shrink\": 0.8}  # Shrink color bar for better layout\n",")\n","\n","# Customize plot labels and title\n","plt.title('Correlation Matrix: Glucose, Bolus, and Infusion', fontsize=16)\n","plt.xticks(rotation=45, fontsize=12)\n","plt.yticks(rotation=45, fontsize=12)\n","\n","# Display the plot\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imFj45WFKi31"},"outputs":[],"source":["# Import the required function\n","from scipy.stats import ttest_ind\n","\n","# Separate groups\n","infusion_group = insulin_data[insulin_data['INPUT_HRS'] > 0]['GLC_ALL']  # Glucose levels for patients on infusion\n","bolus_group = insulin_data[insulin_data['INPUT'] > 0]['GLC_ALL']  # Glucose levels for patients on bolus\n","\n","# Perform t-test\n","t_stat, p_value = ttest_ind(infusion_group, bolus_group, equal_var=False)  # Assuming unequal variances\n","print(f\"T-Test Results: t-statistic = {t_stat}, p-value = {p_value}\")\n","\n","# Interpret results\n","if p_value < 0.05:\n","    print(\"There is a significant difference in glucose levels between patients on infusion vs. bolus insulin.\")\n","else:\n","    print(\"No significant difference in glucose levels between patients on infusion vs. bolus insulin.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1nOB2g6LOo-"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","# Prepare the data\n","# Map categories to numerical values, handling potential NaN values\n","insulin_data['GlucoseCategoryEncoded'] = insulin_data['GlucoseCategory'].map({\n","    'Hypoglycemic': 0,\n","    'Euglycemic': 1,\n","    'Hyperglycemic': 2\n","})\n","\n","# Select features and target\n","features = ['INPUT', 'INPUT_HRS']  # Bolus and infusion insulin rates\n","target = 'GlucoseCategoryEncoded'\n","\n","X = insulin_data[features]\n","y = insulin_data[target]\n","\n","# Impute NaN values with the mean of each column using SimpleImputer\n","from sklearn.impute import SimpleImputer\n","imputer = SimpleImputer(strategy='mean') # You can also use 'median' or other strategies\n","X = imputer.fit_transform(X)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","\n","# Drop rows with NaN values in the target variable 'y'\n","# Create a boolean mask indicating rows with NaN in 'y'\n","nan_mask = y.isna()\n","\n","# Filter out rows with NaN in both X and y\n","X_scaled = X_scaled[~nan_mask]\n","y = y[~nan_mask]\n","\n","# Split data into train and test sets (after dropping NaNs)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# Check class distribution in y_train\n","print(\"Class distribution in training data:\", np.bincount(y_train.astype(int)))  # Convert to int for bincount\n","\n","# Ensure all classes are present in training data\n","classes = np.unique(np.concatenate((y_train, y_test)))  # Get all unique classes from both train and test\n","\n","# Calculate class weights to address class imbalance\n","# Assuming 'balanced' strategy, adjust as needed\n","from sklearn.utils.class_weight import compute_class_weight\n","class_weight_dict = dict(enumerate(compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n","\n","\n","# Train the logistic regression model\n","from sklearn.linear_model import LogisticRegression # Import LogisticRegression\n","model = LogisticRegression(\n","    multi_class='multinomial', solver='lbfgs', max_iter=1000, class_weight=class_weight_dict\n",")\n","model.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = model.predict(X_test)\n","\n","# Classification report with explicit labels\n","print(\"Classification Report:\")\n","print(classification_report(\n","    y_test, y_pred,\n","    labels=[0, 1, 2],\n","    target_names=['Hypoglycemic', 'Euglycemic', 'Hyperglycemic']\n","))\n","\n","# Confusion matrix\n","from sklearn.metrics import confusion_matrix, classification_report # Import confusion_matrix, classification_report\n","conf_matrix = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(\n","    conf_matrix,\n","    annot=True,\n","    fmt='d',\n","    cmap='Blues',\n","    xticklabels=['Hypoglycemic', 'Euglycemic', 'Hyperglycemic'],\n","    yticklabels=['Hypoglycemic', 'Euglycemic', 'Hyperglycemic']\n",")\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"yVu0iiDtkfZw"},"source":["# Inspect the Noteevents dataset"]},{"cell_type":"markdown","metadata":{"id":"hqyGi7YmHBFx"},"source":["**Description of fields:**\n","\n","\n","*   **ROW_ID:** Unique identifier for each row in the dataset.\n","*   **SUBJECT_ID:** It is the unique identifier for an individual patient.\n","*  **HADM_ID:** Represents a single patient‚Äôs admission to the hospital.\n","*   **CHARTDATE:** Gathers the the date on which the note was recorded or created.\n","*   **CHARTTIME:** The specific time (if available) when the note was recorded or created. This column often contains NaN for notes where the exact time isn't recorded.\n","*   **STORETIME:** The time when the note was stored in the database (if available). Often NaN in this dataset.\n","*   **CATEGORY:** The category of the clinical note (e.g., \"Discharge summary,\" \"Nursing,\" etc.), indicating the type or source of the note.\n","*   **DESCRIPTION:** Additional descriptive information about the note (e.g., \"Report\"). Typically, it provides a summary of the note's content or purpose.\n","*   **CGID:** Identifier for the caregiver who wrote the note, if available. Often NaN if the caregiver's ID isn't recorded.\n","*   **ISERROR:** Indicator of whether the note contains an error. Often NaN if errors aren't explicitly flagged.\n","*   **TEXT:** The full text content of the clinical note, including details about the patient's condition, treatment, and other relevant observations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgepgQTWYfjF"},"outputs":[],"source":["# Display the first 5 rows\n","print(noteevents.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NLNz-kmjYtoW"},"outputs":[],"source":["# Display column data types\n","print(noteevents.dtypes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afL_FOOGhyN3"},"outputs":[],"source":["print(f\"Missing CHARTTIME in noteevents: {noteevents['CHARTTIME'].isna().sum()} out of {len(noteevents)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srRS9cihaJzA"},"outputs":[],"source":["# Plot the distribution of note categories\n","noteevents['CATEGORY'].value_counts().plot(kind='bar')\n","plt.title('Distribution of Note Categories')\n","plt.xlabel('Category')\n","plt.ylabel('Count')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qAz4ykszRShM"},"outputs":[],"source":["# View unique values in a column\n","print(noteevents['CATEGORY'].unique())  # List of unique categories\n","\n","# View value counts for a column\n","print(noteevents['TEXT'].value_counts())\n"]},{"cell_type":"code","source":["# Filter noteevents DataFrame\n","categories_to_keep = ['Discharge summary', 'Nursing', 'Physician', 'Nutrition', 'Nursing/other']\n","noteevents = noteevents[noteevents['CATEGORY'].isin(categories_to_keep)]"],"metadata":{"id":"gcKx6huh_yP7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOGb4hckRyXl"},"outputs":[],"source":["# Display a sample of note text\n","print(noteevents['TEXT'].iloc[0])  # First note\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-HFGR_ySfGo"},"outputs":[],"source":["# Display a sample of note text\n","print(noteevents['TEXT'].iloc[1000])  # 1000th note"]},{"cell_type":"markdown","metadata":{"id":"tb4IxWt3kW4L"},"source":["#PreProcess Noteevents Data"]},{"cell_type":"markdown","metadata":{"id":"94rwq4pKsgUe"},"source":["## Filter and format notes data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tJK8IhKuRNJ"},"outputs":[],"source":["def preprocess_text(text):\n","    \"\"\"Preprocess clinical text.\"\"\"\n","    if pd.isna(text):  # Handle NaN values\n","        return \"\"\n","    text = text.lower()\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","    tokens = text.split()\n","    tokens = [word for word in tokens if word not in stop_words]\n","    return ' '.join(tokens).strip()"]},{"cell_type":"markdown","metadata":{"id":"jhVvI1dOssbp"},"source":["# Merge glucose/insulin and notes data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzxrRt0BJ-BO"},"outputs":[],"source":["\n","\n","\n","# Combine 'CHARTDATE' and 'CHARTTIME' into a new 'DATETIME' column\n","noteevents['DATETIME'] = pd.to_datetime(noteevents['CHARTDATE'].astype(str) + ' ' + noteevents['CHARTTIME'].astype(str), errors='coerce')\n","\n","# Convert 'DATETIME' column to datetime objects, handling errors\n","noteevents['DATETIME'] = pd.to_datetime(noteevents['DATETIME'], errors='coerce')\n","\n","\n","# Ensure DATETIME is timezone-naive\n","noteevents['DATETIME'] = noteevents['DATETIME'].dt.tz_localize(None)\n","\n","# Extract CHARTDATE (date-only) from DATETIME\n","noteevents['CHARTDATE'] = noteevents['DATETIME'].dt.date\n","\n","\n","# Ensure DATETIME in filtered_notes is timezone-naive\n","noteevents['DATETIME'] = pd.to_datetime(noteevents['DATETIME'], errors='coerce').dt.tz_localize(None)\n","\n","# Extract CHARTDATE (date-only) from DATETIME in filtered_notes\n","noteevents['CHARTDATE'] = noteevents['DATETIME'].dt.date\n","\n","# Ensure TIMER in insulin_data is timezone-naive\n","insulin_data['TIMER'] = pd.to_datetime(insulin_data['TIMER'], errors='coerce').dt.tz_localize(None)\n","\n","# Extract CHARTDATE (date-only) from TIMER in insulin_data\n","insulin_data['CHARTDATE'] = insulin_data['TIMER'].dt.date\n","\n","# Separate valid and invalid HADM_ID rows in filtered_notes\n","valid_hadm_notes = noteevents[noteevents['HADM_ID'] != -1].copy()\n","invalid_hadm_notes = noteevents[noteevents['HADM_ID'] == -1].copy()\n","\n","# Sort both datasets by CHARTDATE for merge_asof\n","valid_hadm_notes = valid_hadm_notes.sort_values('CHARTDATE')\n","invalid_hadm_notes = invalid_hadm_notes.sort_values('CHARTDATE')\n","insulin_data = insulin_data.sort_values('CHARTDATE')\n","\n","# Merge valid HADM_ID rows on SUBJECT_ID, HADM_ID, and CHARTDATE\n","merged_valid = pd.merge(\n","    valid_hadm_notes,\n","    insulin_data,\n","    on=['SUBJECT_ID', 'HADM_ID', 'CHARTDATE'],  # Match on same day\n","    how='inner'  # Only include rows where all keys match\n",")\n","\n","# Merge invalid HADM_ID rows on SUBJECT_ID and CHARTDATE only\n","merged_invalid = pd.merge(\n","    invalid_hadm_notes,\n","    insulin_data,\n","    on=['SUBJECT_ID', 'CHARTDATE'],  # Match on same day\n","    how='inner'  # Only include rows where all keys match\n",")\n","\n","# Combine the results\n","merged_data = pd.concat([merged_valid, merged_invalid], ignore_index=True)\n","\n","# Verify the structure and content of the merged dataset\n","print(f\"Final merged dataset shape: {merged_data.shape}\")\n","print(merged_data.info())\n","print(merged_data.head())\n"]},{"cell_type":"markdown","metadata":{"id":"wpAWCuqnMzvN"},"source":["# Label data"]},{"cell_type":"markdown","metadata":{"id":"5HD5Z-HqnhjW"},"source":["## Create UMLS dictionary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ujGdWl0njReJ"},"outputs":[],"source":["# Define synonym mapping with CUIs\n","synonym_dict = {\n","    \"BLOOD\": {\"CUI\": \"C0184597\", \"Synonyms\": [\"serum glucose\", \"plasma glucose\", \"glucose level\", \"glucose measurement\"]},\n","    \"FINGERSTICK\": {\"CUI\": \"C0184597\", \"Synonyms\": [\"blood sugar\", \"FBG\", \"BG\", \"blood glucose\", \"random glucose\", \"post-prandial insulin\", \"glucose reading\"]},\n","    \"Hypoglycemic\": {\"CUI\": \"C0020615\", \"Synonyms\": [\"low blood sugar\", \"low glucose\", \"hypo\", \"hypoglycemia\"]},\n","    \"Hyperglycemic\": {\"CUI\": \"C0020456\", \"Synonyms\": [\"high blood sugar\", \"high glucose\", \"hyper\", \"hyperglycemia\"]},\n","    \"Euglycemic\": {\"CUI\": \"C0006681\", \"Synonyms\": [\"normal blood sugar\", \"normal glucose\", \"euglycemia\"]},\n","    \"Bolus\": {\"CUI\": \"C0201974\", \"Synonyms\": [\"bolus insulin\", \"bolus push\", \"SSI\"]},\n","    \"Infusion\": {\"CUI\": \"C0202076\", \"Synonyms\": [\"continuous insulin infusion\", \"IV insulin\", \"intravenous insulin\"]},\n","    \"Short-acting\": {\"CUI\": \"C0201974\", \"Synonyms\": [\"short-acting insulin\", \"rapid-acting insulin\", \"regular insulin\"]},\n","    \"Intermediate-acting\": {\"CUI\": \"C0201982\", \"Synonyms\": [\"intermediate-acting insulin\", \"NPH insulin\"]},\n","    \"Long-acting\": {\"CUI\": \"C0201985\", \"Synonyms\": [\"long-acting insulin\", \"detemir\", \"glargine\"]},\n","    \"No insulin adjustment\": {\"CUI\": \"C0037595\", \"Synonyms\": [\"no change in insulin dose\", \"no insulin adjustment\", \"no dose changed\", \"stable insulin rate\", \"no change in insulin rate\"]},\n","}\n","\n","# Collect all synonyms\n","all_synonyms = []\n","for concept, data in synonym_dict.items():\n","    for phrase in data[\"Synonyms\"]:\n","        all_synonyms.append({\"label\": concept.upper(), \"pattern\": phrase})\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSHZCIx0S2Q4"},"outputs":[],"source":["# Add EntityRuler to pre-tag important synonym phrases\n","ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n","ruler.add_patterns(all_synonyms)\n","\n","# Add UMLS entity linker to pipeline\n","linker = UmlsEntityLinker(resolve_abbreviations=True, name=\"umls\")\n","nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n","\n","# Define CUI filter terms from dictionary\n","target_cuis = {entry[\"CUI\"] for entry in synonym_dict.values()}"]},{"cell_type":"code","source":["# Function to annotate and cache notes, log to wandb\n","def annotate_notes_with_umls(clinical_notes, subject_ids=None, allowed_cuis=None, min_score=0.75, artifact_name=\"umls_annotations\", artifact_version=\"v1\"):\n","    cache_path = Path(\"cache/umls_output.parquet\")\n","    if cache_path.exists():\n","        umls_df = pd.read_parquet(cache_path)\n","    else:\n","        results = []\n","        subjects = subject_ids if subject_ids is not None else [None] * len(clinical_notes)\n","        for i, (doc, subject_id) in enumerate(zip(nlp.pipe(clinical_notes, batch_size=50), subjects)):\n","            for ent in doc.ents:\n","                for umls_ent in ent._.kb_ents:\n","                    cui = umls_ent[0]\n","                    score = umls_ent[1]\n","                    if (allowed_cuis and cui not in allowed_cuis) or (score < min_score):\n","                        continue\n","                    name = linker.kb.cui_to_entity[cui].canonical_name\n","                    concept_label = ent.label_ if ent.label_ in synonym_dict else None\n","                    results.append({\n","                        \"note_id\": i,\n","                        \"subject_id\": subject_id,\n","                        \"entity_text\": ent.text,\n","                        \"start_char\": ent.start_char,\n","                        \"end_char\": ent.end_char,\n","                        \"umls_cui\": cui,\n","                        \"umls_name\": name,\n","                        \"score\": score,\n","                        \"concept_label\": concept_label\n","                    })\n","        umls_df = pd.DataFrame(results)\n","        cache_path.parent.mkdir(parents=True, exist_ok=True)\n","        umls_df.to_parquet(cache_path)\n"],"metadata":{"id":"YYnuynjMD39L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["    wandb.log({\"umls_df\": wandb.Table(dataframe=umls_df)})"],"metadata":{"id":"_Ov47ypgNyNm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["    summary_df = summarize_cui_counts(umls_df)\n","    concept_counts = summary_df.groupby(\"concept_label\")[\"count\"].sum().sort_values(ascending=False).reset_index()\n","    fig, ax = plt.subplots(figsize=(10, 5))\n","    sns.barplot(data=concept_counts, x=\"concept_label\", y=\"count\", ax=ax)\n","    plt.xticks(rotation=45, ha=\"right\")\n","    plt.title(\"Total Mentions per Concept Label\")\n","    plt.tight_layout()\n","    wandb.log({\"concept_distribution\": wandb.Image(fig)})\n","\n","    heatmap_fig, heatmap_ax = plt.subplots(figsize=(12, 8))\n","    pivot = summary_df.pivot_table(index=\"subject_id\", columns=\"concept_label\", values=\"count\", fill_value=0)\n","    sns.heatmap(pivot, annot=False, cmap=\"Blues\", ax=heatmap_ax)\n","    plt.title(\"Concept Frequency per Patient\")\n","    plt.tight_layout()\n","    wandb.log({\"concept_heatmap\": wandb.Image(heatmap_fig)})\n","\n","    artifact = wandb.Artifact(artifact_name, type=\"dataset\", metadata={\"version\": artifact_version, \"min_score\": min_score, \"note_count\": len(clinical_notes)})\n","    artifact.add_file(str(cache_path))\n","    wandb.log_artifact(artifact)\n","\n","    return umls_df"],"metadata":{"id":"Mqjur8OaNvz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def summarize_cui_counts(df):\n","    if df.empty:\n","        return pd.DataFrame(columns=[\"subject_id\", \"concept_label\", \"count\"])\n","    return df.groupby([\"subject_id\", \"concept_label\"]).size().reset_index(name=\"count\")"],"metadata":{"id":"eVj0LiZLUkW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Usage example - integrate with merged_data\n","notes = merged_data[\"TEXT\"].fillna(\"\").astype(str).tolist()\n","subjects = merged_data[\"SUBJECT_ID\"].tolist()\n","\n","umls_df = annotate_notes_with_umls(\n","    clinical_notes=notes,\n","    subject_ids=subjects,\n","    allowed_cuis=target_cuis,\n","    min_score=0.75\n",")"],"metadata":{"id":"TrU11lD7Omqk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combine UMLS annotations back into merged_data\n","umls_grouped = umls_df.groupby(\"subject_id\").agg({\n","    \"umls_cui\": lambda x: list(set(x)),\n","    \"concept_label\": lambda x: list(set(filter(None, x)))\n","}).reset_index()\n","\n","umls_grouped.rename(columns={\n","    \"umls_cui\": \"UMLS_Annotations\",\n","    \"concept_label\": \"UMLS_Annotations_Labels\"\n","}, inplace=True)\n","\n","merged_data = merged_data.merge(umls_grouped, how=\"left\", left_on=\"SUBJECT_ID\", right_on=\"subject_id\")\n","\n","label_to_id = {label: i+1 for i, label in enumerate(synonym_dict.keys())}\n","id_to_label = {v: k for k, v in label_to_id.items()}\n","merged_data[\"UMLS_Annotations_Numeric\"] = merged_data[\"UMLS_Annotations_Labels\"].apply(\n","    lambda labels: [label_to_id.get(lbl, 0) for lbl in labels] if isinstance(labels, list) else []\n",")"],"metadata":{"id":"j2RrIWKzOrv2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vveAwbnT11T4"},"source":["# NER Modelling"]},{"cell_type":"code","source":["# Prepare data for NER\n","texts = merged_data[\"TEXT\"].apply(str.split).tolist()\n","labels = merged_data[\"UMLS_Annotations_Numeric\"].tolist()"],"metadata":{"id":"olf_qHtbUwmk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize ClinicalBERT tokenizer and device\n","tokenizer = BertTokenizerFast.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"],"metadata":{"id":"sDTsIUoBYazA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QuRm-oB6QmjG"},"source":["## Data Tokenization and Labeling"]},{"cell_type":"markdown","metadata":{"id":"AiT4_VqNWcj-"},"source":["### Tokenize"]},{"cell_type":"code","source":["# Function to align labels with tokenized inputs\n","def align_labels_with_tokens(labels, word_ids):\n","    aligned_labels = []\n","    current_word_idx = 0\n","    for word_id in word_ids:\n","        if word_id is None:\n","            aligned_labels.append(-100)  # Ignore special tokens\n","        elif word_id == current_word_idx:\n","            aligned_labels.append(labels[word_id] if word_id < len(labels) else -100)\n","            current_word_idx += 1\n","        else:\n","            aligned_labels.append(labels[current_word_idx - 1] if current_word_idx > 0 else -100)\n","    return aligned_labels\n"],"metadata":{"id":"6hdiAdXABiXY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to tokenize text and align labels\n","def tokenize_and_align_labels(texts, labels):\n","    tokenized_inputs = tokenizer(\n","        texts,\n","        padding=True,\n","        truncation=True,\n","        max_length=128,  # Adjust as needed\n","        return_tensors=\"pt\",\n","        is_split_into_words=True,\n","    )\n","    aligned_labels_list = []\n","    for i, label in enumerate(labels):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        aligned_labels = align_labels_with_tokens(label, word_ids)\n","        aligned_labels_list.append(aligned_labels)\n","    return tokenized_inputs, aligned_labels_list\n","\n","    tokenized_inputs, aligned_labels = tokenize_and_align_labels(texts, labels)"],"metadata":{"id":"AOngJCgXYj5k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ClinicalNERDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","    def __getitem__(self, idx):\n","        return {\n","            \"input_ids\": self.encodings[\"input_ids\"][idx],\n","            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n","            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n","        }\n","    def __len__(self):\n","        return len(self.labels)\n","\n","dataset = ClinicalNERDataset(tokenized_inputs, aligned_labels)\n","train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\n","train_dataset = Subset(dataset, train_idx)\n","test_dataset = Subset(dataset, test_idx)\n"],"metadata":{"id":"nNweAKpQVByv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training function\n","def train_ner_model(train_dataset, model, learning_rate=5e-5, epochs=3):\n","    training_args = TrainingArguments(\n","        output_dir=\"./clinicalbert_results\",\n","        num_train_epochs=epochs,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=16,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        logging_dir=\"./clinicalbert_logs\",\n","        logging_steps=10,\n","        learning_rate=learning_rate,\n","        load_best_model_at_end=True,\n","    )\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        tokenizer=tokenizer,\n","    )\n","    trainer.train()\n","    return trainer"],"metadata":{"id":"D9-xPFAjY1p_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate ClinicalBERT\n","predictions, labels, _ = trainer.predict(test_dataset)\n","predictions = predictions.argmax(axis=-1)\n","\n","true_labels = []\n","pred_labels = []\n","for pred, label in zip(predictions, labels):\n","    for p, l in zip(pred, label):\n","        if l != -100:\n","            true_labels.append(id_to_label[l])\n","            pred_labels.append(id_to_label[p])\n","\n","report = classification_report(true_labels, pred_labels, zero_division=0)\n","accuracy = accuracy_score(true_labels, pred_labels)\n","precision = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n","recall = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n","f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n","\n","print(\"NER Model Evaluation:\")\n","print(report)\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1 Score: {f1:.4f}\")\n"],"metadata":{"id":"T2CIWedsVFKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","import torch.nn as nn\n","\n","# Check if GPU is available, otherwise use CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\") # Print the device being used\n","\n","\n","# Number of epochs\n","num_epochs = 3  # Adjust as needed\n","\n","# Move model to the correct device\n","model.to(device)\n","\n","# Loss tracker\n","training_loss = []\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","\n","    # Use dataloader instead of data_loader in the loop\n","    for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"): # Change data_loader to dataloader\n","        # Move batch to device\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        # Zero gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","\n","        # Compute loss\n","        loss = outputs.loss\n","        epoch_loss += loss.item()\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","    # Track and log loss\n","    avg_loss = epoch_loss / len(dataloader) # Change data_loader to dataloader\n","    training_loss.append(avg_loss)\n","    print(f\"Epoch {epoch + 1} Loss: {avg_loss:.4f}\")"],"metadata":{"id":"XjgSez_3NWIQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YevaHNOWRg07"},"source":["## Split data and Create dataset for Training"]},{"cell_type":"markdown","metadata":{"id":"CGo9ChgURqN9"},"source":["## Train ClinicalBert for NER"]},{"cell_type":"markdown","metadata":{"id":"Gp6ciN2LpImU"},"source":["## Evaluate the NER Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ow6Lf7d2KoMC"},"outputs":[],"source":["from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n","\n","# Evaluation function\n","def evaluate_ner_model(trainer, test_dataset, id_to_label):\n","    predictions, labels, _ = trainer.predict(test_dataset)\n","    predictions = predictions.argmax(axis=-1)\n","\n","    true_labels = []\n","    pred_labels = []\n","\n","    for pred, label in zip(predictions, labels):\n","        for p, l in zip(pred, label):\n","            if l != -100:\n","                true_labels.append(id_to_label[l])\n","                pred_labels.append(id_to_label[p])\n","\n","    report = classification_report(true_labels, pred_labels, zero_division=0)\n","    accuracy = accuracy_score(true_labels, pred_labels)\n","    precision = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n","    recall = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n","    f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n","\n","    print(\"NER Model Evaluation:\")\n","    print(report)\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","\n","# Evaluate Model\n","evaluate_ner_model(trainer, test_dataset, id_to_label)\n"]},{"cell_type":"markdown","metadata":{"id":"JCV_Us8c1UVe"},"source":["\n","# Relation Extraction"]},{"cell_type":"code","source":["def prepare_relation_data_fixed(data, umls_dict):\n","    pairs, labels = [], []\n","    for _, row in data.iterrows():\n","        entities = row['UMLS_Annotations']\n","        if not entities:\n","            continue\n","        for i, entity1 in enumerate(entities):\n","            for j, entity2 in enumerate(entities):\n","                if i < j:  # Avoid redundancy (entity1, entity2) == (entity2, entity1)\n","                    pairs.append((entity1, entity2))\n","                    labels.append(1 if entity1 == entity2 else 0)\n","    return pairs, labels\n","\n","pairs, labels = prepare_relation_data(merged_data, umls_dict)\n","\n","# Create a custom dataset class\n","class RelationDataset(Dataset):\n","    def __init__(self, pairs, labels, tokenizer):\n","        self.pairs = pairs\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, idx):\n","        entity1, entity2 = self.pairs[idx]  # Unpack the tuple\n","        label = self.labels[idx]\n","\n","        # Tokenize the pair using the [SEP] token\n","        encoding = self.tokenizer(\n","            entity1,\n","            entity2,\n","            add_special_tokens=True,\n","            padding='max_length',\n","            truncation=True,\n","            max_length=128,  # Adjust as needed\n","            return_tensors='pt'\n","        )\n","\n","        # Return a dictionary\n","        return {\n","            'input_ids': encoding['input_ids'].squeeze(),\n","            'attention_mask': encoding['attention_mask'].squeeze(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# Initialize tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n","\n","# Create Dataset objects\n","train_dataset = RelationDataset(pairs, labels, tokenizer)\n","# Assuming you want to use the same data for evaluation,\n","# you can create a separate eval_dataset or use train_dataset\n","eval_dataset = RelationDataset(pairs, labels, tokenizer)\n","\n","# Use BertForSequenceClassification for relation extraction\n","from transformers import BertForSequenceClassification # Import BertForSequenceClassification\n","relation_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","\n","relation_training_args = TrainingArguments(\n","    output_dir=\"./relation_results\",\n","    evaluation_strategy=\"epoch\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=64\n",")\n","relation_trainer = Trainer(\n","    model=relation_model,\n","    args=relation_training_args,\n","    train_dataset=train_dataset,  # Pass the Dataset object\n","    eval_dataset=eval_dataset     # Pass the Dataset object\n",")\n","relation_trainer.train"],"metadata":{"id":"sxLBhtth3_lL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["relation_eval = relation_trainer.evaluate()\n","print(\"Relation Extraction Results:\", relation_eval)\n","\n","# Visualize RE Results\n","plt.figure(figsize=(10, 6))\n","sns.histplot([f\"{pair}: {label}\" for pair, label in zip(pairs, labels)], kde=False)\n","plt.title('Common Relationships')\n","plt.xlabel('Relationship')\n","plt.ylabel('Frequency')\n","plt.show()"],"metadata":{"id":"Oi3u0r6N_Ny3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset\n","import torch\n","\n","# Initialize ClinicalBERT tokenizer and device\n","tokenizer = BertTokenizerFast.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Function to align labels with tokenized inputs\n","def align_labels_with_tokens(labels, word_ids):\n","    aligned_labels = []\n","    current_word_idx = 0\n","    for word_id in word_ids:\n","        if word_id is None:\n","            aligned_labels.append(-100)  # Ignore special tokens\n","        elif word_id == current_word_idx:\n","            aligned_labels.append(labels[word_id] if word_id < len(labels) else -100)\n","            current_word_idx += 1\n","        else:\n","            aligned_labels.append(labels[current_word_idx - 1] if current_word_idx > 0 else -100)\n","    return aligned_labels\n","\n","# Function to tokenize text and align labels\n","def tokenize_and_align_labels(texts, labels):\n","    tokenized_inputs = tokenizer(\n","        texts,\n","        padding=True,\n","        truncation=True,\n","        max_length=128,  # Adjust as needed\n","        return_tensors=\"pt\",\n","        is_split_into_words=True,\n","    )\n","    aligned_labels_list = []\n","    for i, label in enumerate(labels):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        aligned_labels = align_labels_with_tokens(label, word_ids)\n","        aligned_labels_list.append(aligned_labels)\n","    return tokenized_inputs, aligned_labels_list\n","\n","# Dataset class\n","class NERDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return item\n","\n","# Example texts and labels\n","texts = [\n","    \"Patient presented with hyperglycemia.\",\n","    \"Administered insulin and rechecked glucose levels.\",\n","    \"The glucose reading was 150 mg/dL.\",\n","    \"No signs of hypoglycemia were noted.\",\n","]\n","labels = [\n","    [0, 3, 0, 0, 4, 0],   # Example labels for the first sentence\n","    [0, 6, 0, 0, 0, 4, 0],  # Example labels for the second sentence\n","    [0, 0, 4, 0, 0, 0, 0, 0],  # Example labels for the third sentence\n","    [0, 0, 0, 3, 0, 0],   # Example labels for the fourth sentence\n","]\n","\n","# Train-Test Split\n","texts_train, texts_test, labels_train, labels_test = train_test_split(\n","    texts, labels, test_size=0.2, random_state=42\n",")\n","\n","# Tokenize and align labels\n","train_encodings, train_labels = tokenize_and_align_labels(texts_train, labels_train)\n","test_encodings, test_labels = tokenize_and_align_labels(texts_test, labels_test)\n","\n","# Prepare datasets\n","train_dataset = NERDataset(train_encodings, train_labels)\n","test_dataset = NERDataset(test_encodings, test_labels)\n","\n","# Load ClinicalBERT model\n","num_labels = 12 # Replace with the actual number of labels\n","model = BertForTokenClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=num_labels)\n","model.to(device)\n","\n","# Training function\n","def train_ner_model(train_dataset, test_dataset, model, learning_rate=5e-5, epochs=3):\n","    training_args = TrainingArguments(\n","        output_dir=\"./clinicalbert_results\",\n","        num_train_epochs=epochs,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=16,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        logging_dir=\"./clinicalbert_logs\",\n","        logging_steps=10,\n","        learning_rate=learning_rate,\n","        load_best_model_at_end=True,\n","    )\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=test_dataset,\n","        tokenizer=tokenizer,\n","    )\n","    trainer.train()\n","    return trainer\n","\n","# Train the model\n","trainer = train_ner_model(train_dataset, test_dataset, model, learning_rate=5e-5, epochs=3)\n"],"metadata":{"id":"tOYf4OBBRiDU"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}